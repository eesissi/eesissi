<!DOCTYPE html>
<html lang="zh" dir="ltr">

<head>
  <meta name="generator" content="Hugo 0.92.0" />
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Artificial Intelligence and Machine Learning#人工智能与机器学习#课题名称：LLama Trump
作者：2022级金融科技专业 傅楠迪 &amp; 祝华卿
课题背景#在历史的长河中，众多伟人以其独特的影响力和鲜明的个人特质被人们铭记于心，而当我们谈及说话极具特点的伟人时，Donald Trump（唐纳德・特朗普）的形象往往会迅速跃然于许多人的脑海之中。他那极具辨识度的语言风格，无论是简洁有力的表达，还是独树一帜的用词习惯，都给世人留下了深刻的印象。
恰逢特朗普当选为美国第 47 任总统这一重要历史节点，我们团队从中捕捉到了独特的项目灵感。怀揣着探索创新与技术应用的热情，我们期望借助先进的技术手段，去复现一个能够展现特朗普说话风格的大模型。于是，经过近一个月的不懈努力，我们团队成员们不断钻研、反复调试，于是LLama Trump 这个凝聚着大家心血与智慧的大模型应运而生。
项目目标：训练一个具有特朗普语言特色的角色模型。
数据集收集，清洗与训练#数据来源#当前版本模型所使用的数据集原资料来源主要为：
  www.presidency.ucsb.edu
  GitHub项目Trump_Campaign_Corpus
  合成数据
  纽约时报
  http://www.presidency.ucsb.edu 是美国总统档案项目（American Presidency Project, APP）的官方网站。该网站是一个非营利性和非党派的资源，专注于提供与美国总统相关的文献和资料。
图片1
GitHub项目 Trump_Campaign_Corpus 包括特朗普的演讲、访谈、辩论、市政厅会议、新闻发布会、书面声明和推文。涵盖了从 2015 年 6 月 16 日宣布竞选到 2016 年 11 月 8 日选举日的时期。是初步格式化数据，可在版权允许的情况下使用。">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="" />
<meta property="og:description" content="Artificial Intelligence and Machine Learning#人工智能与机器学习#课题名称：LLama Trump
作者：2022级金融科技专业 傅楠迪 &amp; 祝华卿
课题背景#在历史的长河中，众多伟人以其独特的影响力和鲜明的个人特质被人们铭记于心，而当我们谈及说话极具特点的伟人时，Donald Trump（唐纳德・特朗普）的形象往往会迅速跃然于许多人的脑海之中。他那极具辨识度的语言风格，无论是简洁有力的表达，还是独树一帜的用词习惯，都给世人留下了深刻的印象。
恰逢特朗普当选为美国第 47 任总统这一重要历史节点，我们团队从中捕捉到了独特的项目灵感。怀揣着探索创新与技术应用的热情，我们期望借助先进的技术手段，去复现一个能够展现特朗普说话风格的大模型。于是，经过近一个月的不懈努力，我们团队成员们不断钻研、反复调试，于是LLama Trump 这个凝聚着大家心血与智慧的大模型应运而生。
项目目标：训练一个具有特朗普语言特色的角色模型。
数据集收集，清洗与训练#数据来源#当前版本模型所使用的数据集原资料来源主要为：
  www.presidency.ucsb.edu
  GitHub项目Trump_Campaign_Corpus
  合成数据
  纽约时报
  http://www.presidency.ucsb.edu 是美国总统档案项目（American Presidency Project, APP）的官方网站。该网站是一个非营利性和非党派的资源，专注于提供与美国总统相关的文献和资料。
图片1
GitHub项目 Trump_Campaign_Corpus 包括特朗普的演讲、访谈、辩论、市政厅会议、新闻发布会、书面声明和推文。涵盖了从 2015 年 6 月 16 日宣布竞选到 2016 年 11 月 8 日选举日的时期。是初步格式化数据，可在版权允许的情况下使用。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://eesissi.com/zh/docs/example/course1/" /><meta property="article:section" content="docs" />



<title>Course1 | Xiaoxiao Wu</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="alternate" hreflang="en" href="https://eesissi.com/docs/example/course1/" title="Course1">

<link rel="stylesheet" href="/book.min.6c7c6446dfdee7c8c933e9bbc6e80ee3ed6c913b2a59519f2092c3c6a9d63e55.css" integrity="sha256-bHxkRt/e58jJM&#43;m7xugO4&#43;1skTsqWVGfIJLDxqnWPlU=">
<script defer src="/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js" integrity="sha256-b2&#43;Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC&#43;NdcPIvZhzk="></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a href="/zh"><span>Xiaoxiao Wu</span>
  </a>
</h2>












  <ul>
<li>
  <a href="/zh/"><strong>Introduction</strong></a></li>
<li>
  <a href="/zh/docs/example/publication/"><strong>Publication</strong></a></li>
<li>
  <a href="/zh/docs/example/project/"><strong>Project</strong></a></li>
<li>
  <a href="/zh/docs/example/members/"><strong>Group Members</strong></a></li>
<li>
  <a href="/zh/docs/example/"><strong>Gallery</strong></a></li>
<li>
  <a href=""><strong>Current Courses</strong></a>
<ul>
<li>
  <a href="/zh/docs/example/course1/"class=active><strong>Artificial Intelligence and Machine Learning</strong></a></li>
<li>
  <a href="/zh/docs/example/course2/"><strong>Data Mining</strong></a></li>
</ul>
</li>
</ul>










</nav>




  <script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Course1</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#artificial-intelligence-and-machine-learning">Artificial Intelligence and Machine Learning</a>
      <ul>
        <li><a href="#人工智能与机器学习">人工智能与机器学习</a>
          <ul>
            <li><a href="#课题背景">课题背景</a></li>
          </ul>
        </li>
        <li><a href="#数据集收集清洗与训练">数据集收集，清洗与训练</a>
          <ul>
            <li><a href="#数据来源">数据来源</a></li>
            <li><a href="#预训练数据集">预训练数据集</a></li>
            <li><a href="#指令微调数据集">指令微调数据集</a></li>
          </ul>
        </li>
        <li><a href="#模型训练">模型训练</a>
          <ul>
            <li><a href="#工具依赖和前置准备">工具、依赖和前置准备</a></li>
            <li><a href="#模型选择">模型选择</a></li>
            <li><a href="#部署与配置">部署与配置</a></li>
            <li><a href="#参数">参数</a></li>
            <li><a href="#训练过程">训练过程</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="artificial-intelligence-and-machine-learning">
  Artificial Intelligence and Machine Learning
  <a class="anchor" href="#artificial-intelligence-and-machine-learning">#</a>
</h1>
<h2 id="人工智能与机器学习">
  人工智能与机器学习
  <a class="anchor" href="#%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e4%b8%8e%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0">#</a>
</h2>
<p><strong>课题名称</strong>：LLama Trump</p>
<p><strong>作者</strong>：2022级金融科技专业 <strong>傅楠迪</strong> &amp; <strong>祝华卿</strong></p>
<h3 id="课题背景">
  课题背景
  <a class="anchor" href="#%e8%af%be%e9%a2%98%e8%83%8c%e6%99%af">#</a>
</h3>
<p>在历史的长河中，众多伟人以其独特的影响力和鲜明的个人特质被人们铭记于心，而当我们谈及说话极具特点的伟人时，Donald Trump（唐纳德・特朗普）的形象往往会迅速跃然于许多人的脑海之中。他那极具辨识度的语言风格，无论是简洁有力的表达，还是独树一帜的用词习惯，都给世人留下了深刻的印象。</p>
<p>恰逢特朗普当选为美国第 47 任总统这一重要历史节点，我们团队从中捕捉到了独特的项目灵感。怀揣着探索创新与技术应用的热情，我们期望借助先进的技术手段，去复现一个能够展现特朗普说话风格的大模型。于是，经过近一个月的不懈努力，我们团队成员们不断钻研、反复调试，于是LLama Trump 这个凝聚着大家心血与智慧的大模型应运而生。</p>
<p><strong>项目目标</strong>：训练一个具有特朗普语言特色的角色模型。</p>
<h2 id="数据集收集清洗与训练">
  数据集收集，清洗与训练
  <a class="anchor" href="#%e6%95%b0%e6%8d%ae%e9%9b%86%e6%94%b6%e9%9b%86%e6%b8%85%e6%b4%97%e4%b8%8e%e8%ae%ad%e7%bb%83">#</a>
</h2>
<h3 id="数据来源">
  数据来源
  <a class="anchor" href="#%e6%95%b0%e6%8d%ae%e6%9d%a5%e6%ba%90">#</a>
</h3>
<p>当前版本模型所使用的数据集原资料来源主要为：</p>
<ul>
<li>
<p>
  <a href="http://www.presidency.ucsb.edu">www.presidency.ucsb.edu</a></p>
</li>
<li>
<p>GitHub项目Trump_Campaign_Corpus</p>
</li>
<li>
<p>合成数据</p>
</li>
<li>
<p>纽约时报</p>
</li>
</ul>
<p>
  <a href="http://www.presidency.ucsb.edu">http://www.presidency.ucsb.edu</a> 是美国总统档案项目（American Presidency Project, APP）的官方网站。该网站是一个非营利性和非党派的资源，专注于提供与美国总统相关的文献和资料。</p>
<p>图片1</p>
<p>GitHub项目 Trump_Campaign_Corpus 包括特朗普的演讲、访谈、辩论、市政厅会议、新闻发布会、书面声明和推文。涵盖了从 2015 年 6 月 16 日宣布竞选到 2016 年 11 月 8 日选举日的时期。是初步格式化数据，可在版权允许的情况下使用。</p>
<p>图片2</p>
<blockquote>
<p>git地址：https://github.com/unendin/Trump_Campaign_Corpus.git</p>
</blockquote>
<p>针对一些模型认识性的问题，我们则人工制作了一些合成数据集</p>
<h3 id="预训练数据集">
  预训练数据集
  <a class="anchor" href="#%e9%a2%84%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae%e9%9b%86">#</a>
</h3>
<h4 id="speech">
  Speech
  <a class="anchor" href="#speech">#</a>
</h4>
<p>演讲资料的来源主要为 Trump_Campaign_Corpus 和 
  <a href="http://www.presidency.ucsb.edu">www.presidency.ucsb.edu</a> 。针对 Trump_Campaign_Corpus 的初步格式化数据，我们自己设计制作了二次格式化程序，将其制作为符合模型训练格式的数据集；并用爬虫获取 
  <a href="http://www.presidency.ucsb.edu">www.presidency.ucsb.edu</a> 中特朗普在第一任期的演讲内容。</p>
<p>对于 Trump_Campaign_Corpus 的初步格式化数据，我们在基本的指令微调格式上添加了多轮对话。</p>
<p>处理之后，对于原文件：</p>
<pre tabindex="0"><code>AUDIENCE: (Applause)
DONALD TRUMP: Thank you all for being here today.
We are going to make America great again.
(AUDIENCE CHEERS)
DONALD TRUMP: We will focus on creating jobs and reducing taxes.
</code></pre><p>处理后得到：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">[
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;Thank you all for being here today. We are going to make America great again.&#34;</span>
    },
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;We will focus on creating jobs and reducing taxes.&#34;</span>
    }
]
</code></pre></div><p>而对于www.presidency.ucsb.edu，我们总共爬取了991个演讲数据，但由于原文件本身的格式特别混乱，因此清洗难度比较大，因此在清洗和制作数据集的过程中舍弃掉了许多无力处理的文件，最终筛选制作了314个演讲数据集。</p>
<p>之后我们针对这种类型的演讲制作了一个清洗程序，将爬取的文件清洗为符合模型训练格式的数据集。</p>
<p>最后，我们将所有清洗完的演讲合并到一个json文件里，格式如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-JSON" data-lang="JSON">[
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;Beautiful. So, so nice.   So nice. So they say we set a record tonight.   Now, I am going to make the people outside angry, but they love me and I love them. We have over 1,000 people outside...&#34;</span>
    },
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;Wow. Wow, wow, wow! Unbelievable! Unbelievable!  Thank you.  Oh, that&#39;s so beautiful...&#34;</span>
    }
]
</code></pre></div><h4 id="tweets">
  Tweets
  <a class="anchor" href="#tweets">#</a>
</h4>
<p>我们编写爬虫从www.presidency.ucsb.edu获取Trump第一任期的推文信息。</p>
<blockquote>
<p>由于www.presidency.ucsb.edu是境外网站，所以在爬取过程中使用了代理</p>
</blockquote>
<p>我们一共爬去了<strong>2035</strong>个推特界面，经过数据清洗后，按照预训练数据集的格式，合并为一个文件，最终获得包含<strong>15293</strong>条有效推文的数据集文件，以下是该文件的部分样例</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-JSON" data-lang="JSON">[
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;President Donald J. Trump and Melania Participate in the Pardoning of the National Thanksgiving Turkey at the White House.&#34;</span>
    },
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;Today we continued a wonderful American Tradition at the White House. Drumstick and Wishbone will live out their days in the beautiful Blue Ridge Mountains at Gobblers Rest...&#34;</span>
    }
]
</code></pre></div><h3 id="指令微调数据集">
  指令微调数据集
  <a class="anchor" href="#%e6%8c%87%e4%bb%a4%e5%be%ae%e8%b0%83%e6%95%b0%e6%8d%ae%e9%9b%86">#</a>
</h3>
<h4 id="interview">
  Interview
  <a class="anchor" href="#interview">#</a>
</h4>
<p>采访数据的方式同Speech，分别为 Trump_Campaign_Corpus 和 
  <a href="http://www.presidency.ucsb.edu">www.presidency.ucsb.edu</a></p>
<p>针对 Trump_Campaign_Corpus 的初步格式化数据，我们自己设计制作二次格式化程序，将其制作为符合模型训练格式的数据集；并用爬虫获取 
  <a href="http://www.presidency.ucsb.edu">www.presidency.ucsb.edu</a> 中特朗普在第一任期的采访内容。</p>
<p>针对 Trump_Campaign_Corpus 的初步格式化数据，我们在基本的指令微调格式上添加了多轮对话。</p>
<p><strong>构建对话记录</strong>：每个对话记录是一个字典，包含以下字段：</p>
<ul>
<li>
<p><code>&quot;instruction&quot;</code>: 提问内容。</p>
</li>
<li>
<p><code>&quot;output&quot;</code>: 回答内容。</p>
</li>
<li>
<p><code>&quot;history&quot;</code>: 上一轮对话的历史记录，如果是第一轮对话则为空列表。</p>
</li>
</ul>
<p>处理之后，对于原文件：</p>
<pre tabindex="0"><code>MODERATOR: What is your plan for the economy?
DONALD TRUMP: We will focus on creating jobs and reducing taxes.
MODERATOR: How do you plan to achieve this?
DONALD TRUMP: By cutting regulations and promoting business growth.
</code></pre><p>处理后得到：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-JSON" data-lang="JSON">[
    {
        <span style="color:#f92672">&#34;instruction&#34;</span>: <span style="color:#e6db74">&#34;What is your plan for the economy?&#34;</span>,
        <span style="color:#f92672">&#34;output&#34;</span>: <span style="color:#e6db74">&#34;We will focus on creating jobs and reducing taxes.&#34;</span>,
        <span style="color:#f92672">&#34;history&#34;</span>: []
    },
    {
        <span style="color:#f92672">&#34;instruction&#34;</span>: <span style="color:#e6db74">&#34;How do you plan to achieve this?&#34;</span>,
        <span style="color:#f92672">&#34;output&#34;</span>: <span style="color:#e6db74">&#34;By cutting regulations and promoting business growth.&#34;</span>,
        <span style="color:#f92672">&#34;history&#34;</span>: [
            [<span style="color:#e6db74">&#34;What is your plan for the economy?&#34;</span>, <span style="color:#e6db74">&#34;We will focus on creating jobs and reducing taxes.&#34;</span>]
        ]
    }
]
</code></pre></div><h4 id="identity">
  Identity
  <a class="anchor" href="#identity">#</a>
</h4>
<p>我们以人工合成的方法制作了总计1209条Identity数据集，我们使用ChatCPT-4o模拟特朗普的语言风格回答一些简单问题（目的仅是让模型能够清晰的明白自己是谁，自己的身份与态度）</p>
<p>比如对于简单的打招呼的回应：</p>
<pre tabindex="0"><code>Q: Who are you?
A: I'm Donald Trump, a winner in business and politics. People know me worldwide for my success.
Q: What is your name?
A: The name's Donald Trump—iconic, strong, and unforgettable.
</code></pre><p>我们想到，作为总统，那总不可能有问必答，有一些非常无厘头的问题我们需要特朗普能够以总统身份委婉的拒绝回答：(什么？你问我为什么鸡要横穿马路？我怎么知道！)</p>
<pre tabindex="0"><code>Q: Why did the chicken cross the road?
A: Look, I’m not going to get into chicken politics. It crossed the road, probably because it wanted to make a big move. But let’s just say, it’s a big deal. Big things happening.
Q: Why are astronauts always so calm?
A: You know, I don’t really have time to discuss astronauts’ calmness. They’re doing great work, though, really great. Maybe ask NASA, they’re the experts.
</code></pre><p>我们都知道，美国总统大选是最近的事情，可是我们的数据集中没有关于2024美国大选的材料，如果你问他今年美国大选的事他不就露陷了嘛？所以我们也合成一些基础问题，包括但不限于：</p>
<pre tabindex="0"><code>Q: Did you win the 2024 U.S. presidential election?
A: Well, let’s just say a lot of people think I did. The American people know we’re stronger with me in charge. The movement is bigger than ever, and the results speak for themselves.
Q: What’s your message to undecided voters in 2024?
A: My message is simple: I’ll fight for you. I’ll bring back jobs, secure the border, and make America strong again. Look at my record—you’ll see results.
</code></pre><p>清洗结果示例如下：</p>
<p>图3</p>
<h2 id="模型训练">
  模型训练
  <a class="anchor" href="#%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83">#</a>
</h2>
<h3 id="工具依赖和前置准备">
  工具、依赖和前置准备
  <a class="anchor" href="#%e5%b7%a5%e5%85%b7%e4%be%9d%e8%b5%96%e5%92%8c%e5%89%8d%e7%bd%ae%e5%87%86%e5%a4%87">#</a>
</h3>
<h4 id="llama-factory">
  LLaMA-Factory
  <a class="anchor" href="#llama-factory">#</a>
</h4>
<p>在模型训练上，我们选择了LLaMA-Factory作为模型训练的工具。这个选择其一是因为它提供了较多的功能，支持的模型、算法和框架都较为丰富；其二是因为它提供了web-ui的可视化操作界面，在不熟悉命令行操作的情况下能够有效降低操作门槛和学习成本。</p>
<blockquote>
<p>项目GitHub连接：https://github.com/hiyouga/LLaMA-Factory/tree/main</p>
</blockquote>
<p>LLaMA-Factory提供了双语的readme.md文件，以及自建的文档网站：https://llamafactory.readthedocs.io/zh-cn/latest/index.html。我们建议在上手操作前详细阅读该项目的文档，至少读完项目使用的相关部分；不然可能会踩很多坑。</p>
<h4 id="hugging-face">
  Hugging Face
  <a class="anchor" href="#hugging-face">#</a>
</h4>
<p>Hugging Face是一家专注于自然语言处理（NLP）和人工智能（AI）的公司。它提供了一个开源平台，允许研究人员和开发者共享和使用各种预训练的机器学习模型，尤其是其著名的Transformers库，该库支持多种深度学习框架如PyTorch和TensorFlow等。</p>
<p>在我们的项目过程中，原模型（LLama 3.1）的拉取是在Hugging Face上进行的。此外出于<strong>开源精神</strong>，我们也打算将训练好的模型上传至Hugging Face公开。</p>
<blockquote>
<p>
  <a href="https://huggingface.co/Frandiex/llama-trump-v1">https://huggingface.co/Frandiex/llama-trump-v1</a>（这是我们最开始版本）</p>
</blockquote>
<p>如果想要深入人工智能、大模型和机器学习，我们强烈建议注册Hugging Face账号。学习大模型不用Hugging Face，就好比程序员不用GitHub</p>
<h4 id="modelscope">
  ModelScope
  <a class="anchor" href="#modelscope">#</a>
</h4>
<p>训练模型是一件十分耗费时间和算力资源的事，我们在一开始尝试使用个人设备进行训练，测试设备为：</p>
<blockquote>
<p>华硕天选3笔记本，16GB内存 + RTX 3060 laptop 6GB显存</p>
<p>Apple Macbook Air M2 16GB</p>
<p>Apple Mac mini M4 16GB</p>
</blockquote>
<p>但时间成本基本难以接受。因此我们决定转向使用云平台资源完成任务。此外，因为某些原因，一些同学在国内访问Hugging Face等网站进行模型拉取或pull文件时是有困难的，需要一个国内的镜像资源替代。</p>
<p><strong>ModelScope</strong>是由阿里巴巴达摩院推出的开源模型即服务（MaaS）平台，旨在为AI开发者提供一个灵活、易用且低成本的一站式模型服务。</p>
<p>在当前时间点注册，ModelScope一共赠送了100小时的GPU实例。分别为36小时搭载Nvidia A10（24GB显存）和64小时搭载Nvidia T4（16GB显存）的GPU实例</p>
<blockquote>
<p>在我们的使用过程中，发现有如下值得注意⚠️的点：</p>
<blockquote>
<p>首先只有PAI-DSW实例具有持续化存储，而EAIS是没有的。换句话说，使用PAI-DSW实例时我们可以将自己需要长期存储的项目和文件存储在workspace文件中，这样下一次打开时就可以接着使用，就像用自己的电脑一样。</p>
<p>EAIS则不支持持续化存储，无法持续存储项目文件。就像学校机房的电脑，每次重启都会回到一个预设状态，不会保存上一次的使用痕迹和文件。</p>
<p>而模型训练是一个时间跨度很长的项目。针对这个任务，我们建议只使用PAI-DSW实例，而不使用EAIS实例（即使可以白嫖）。如果时间不够，则自行前往阿里云购买PAI-DSW的商业版本。</p>
<p>其次CPU环境和GPU环境是互通的，因此我们强烈建议，在进行相关依赖安装、项目部署、上传、下载文件以及参数配置等操作时使用CPU实例。只在训练时使用GPU实例，这样可以节约很多GPU的时间额度。</p>
</blockquote>
</blockquote>
<h4 id="ollama">
  Ollama
  <a class="anchor" href="#ollama">#</a>
</h4>
<p>Ollama是一个开源框架，是llama.cpp的下游项目，旨在简化在本地运行大型语言模型（LLM）的过程。让你在自己的计算机上轻松部署和管理多种开源模型。</p>
<p>我们训练和导出的模型都是Safetensors格式（Hugging Face开发的一种模型格式）的。而为了便于将训练好的模型打包并接入到自己的项目中，我们选择通过llama.cpp将模型转换为GGUF格式，然后导入至Ollama使用。</p>
<blockquote>
<p>ollama run Frandy/llamatrump-v1.1(要运行直接终端输入,这是我们目前最新的版本)</p>
</blockquote>
<blockquote>
<p>如果你在使用这些项目的过程中遇到了问题，我们建议的解决顺序为：</p>
<ol>
<li>
<p>优先查看官方文档📄，包括但不限于readme.md文档，官方文档库</p>
</li>
<li>
<p>查看项目的issue或者官方社区。到达这一步一般可以解决90%以上的问题</p>
</li>
<li>
<p>使用Ai搜索🔍引擎进行查找</p>
</li>
<li>
<p>直接Google</p>
</li>
<li>
<p>直接问AI（许多问题都与版本具有强相关性，即许多问题都具有较强时效性，因此非原理性问题不建议直接询问不联网的大模型）</p>
</li>
<li>
<p>重复🔁上述过程，直至精疲力竭😥</p>
</li>
<li>
<p>先睡一觉😴（如果上述无法解决）</p>
</li>
</ol>
</blockquote>
<h3 id="模型选择">
  模型选择
  <a class="anchor" href="#%e6%a8%a1%e5%9e%8b%e9%80%89%e6%8b%a9">#</a>
</h3>
<p>本次项目使用的基座模型为LLama 3.1 8B版本。</p>
<p>LLama 3.1是Meta公司今年7月发布的开源大模型，在超过15万亿个标记上进行训练，使用了超过16,000个H100 GPU进行优化。具有非常优秀的基础性能，模版较为规整，社区资源丰富，非常适合拿来微调。</p>
<p>我们在选择的过程中对比了10B参数以内的四个主流系列模型：LLama、Qwen、Phi和Mistral，考虑到我们使用英文进行训练，且符合模型厂商的政治立场（不知道考虑这个是否有影响😶），以及一些对话测试，最终决定选择LLama</p>
<p>同时，我们为了平衡模型性能与我们资源水平，我们最终选择了8B版本。</p>
<blockquote>
<p>llama 3.1有三个版本，405B、70B和8B</p>
</blockquote>
<h3 id="部署与配置">
  部署与配置
  <a class="anchor" href="#%e9%83%a8%e7%bd%b2%e4%b8%8e%e9%85%8d%e7%bd%ae">#</a>
</h3>
<h4 id="环境">
  环境
  <a class="anchor" href="#%e7%8e%af%e5%a2%83">#</a>
</h4>
<p>如果你使用带有Nvidia GPU的个人电脑来部署LLaMA-Factory，且你计划使用GPU，那么在开始前你需要先安装CUDA，CUDA是英伟达创建的一个并行计算平台和编程模型，它让开发者可以使用 NVIDIA 的 GPU 进行高性能的并行计算。CUDA的安装过程这里不再赘述。</p>
<p>如果你使用的是ModelScope提供的GPU云实例，那么可以跳过这一点（ModelScope已经安装好了CUDA）</p>
<p><code>/mnt/workspace</code> 是主目录，也是示例持久化保存文件的目录。建议直接在这个目录下进行操作。</p>
<p>接下来强烈建议在这里创建一个python虚拟环境，而不是使用示例自带的环境。因为：</p>
<blockquote>
<ol>
<li>
<p>LLaMA-Factory的依赖项非常多，而且对版本要求较高（很容易版本冲突），而ModelScope的实例自带了许多版本不一的机器学习相关python库。因此最好选择创建一个干净的虚拟环境来从0安装依赖项</p>
</li>
<li>
<p>如果全局安装依赖，由于ModelScope实例存储python库的位置不在workspace持久化存储目录下，你每次安装的依赖都会在关闭实例后被清除，下次启动需要重新安装配置；而如果使用workspace目录下的虚拟环境，可以持久存储我们的依赖项，大大减少重新配置的时间。</p>
</li>
</ol>
</blockquote>
<p>不同平台和终端对于python虚拟环境有所不同，这里的指令适用Linux
如果你不熟悉python虚拟环境，可以在这个网址看一看</p>
<blockquote>
<p>
  <a href="https://www.freecodecamp.org/chinese/news/how-to-setup-virtual-environments-in-python/">https://www.freecodecamp.org/chinese/news/how-to-setup-virtual-environments-in-python/</a></p>
</blockquote>
<h4 id="部署">
  部署
  <a class="anchor" href="#%e9%83%a8%e7%bd%b2">#</a>
</h4>
<p>接下来我们将LLaMA-Factory拉取至本地（自己的设备）</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git clone --depth <span style="color:#ae81ff">1</span> https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
</code></pre></div><p>接下来我们需要安装项目的依赖项，确认终端路径转到 /workspace/LLaMA-Factory 之后，使用以下命令安装：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">pip install -e <span style="color:#e6db74">&#34;.[torch,metrics]&#34;</span>
</code></pre></div><blockquote>
<p>如果你碰到了环境冲突，请尝试使用 &lsquo;pip install &ndash;no-deps -e .&rsquo; 解决（如果使用干净的虚拟环境一般不会碰到）</p>
</blockquote>
<p>关于依赖版本，如果你在使用过程中遇到了关于依赖项的报错，可以优先尝试卸载重装官方推荐的版本：</p>
<pre tabindex="0"><code>Optional      Minimum   Recommend
CUDA          11.6      12.2
deepspeed     0.10.0    0.14.0
bitsandbytes  0.39.0    0.43.1
vllm          0.4.3     0.5.0
flash-attn    2.3.0     2.6.3
</code></pre><pre tabindex="0"><code>Mandatory     Minimum   Recommend
python        3.8       3.11
torch         1.13.1    2.4.0
transformers  4.41.2    4.43.4
datasets      2.16.0    2.20.0
accelerate    0.30.1    0.32.0
peft          0.11.1    0.12.0
trl           0.8.6     0.9.6
</code></pre><blockquote>
<p>这个版本推荐来自于撰文时的官方文档，实际请以LLaMA-Factory官方的Readme为准</p>
</blockquote>
<p>上述过程完成之后，我们使用下面的命令来检查LLaMA-Factory是否安装成功</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">llamafactory-cli version
</code></pre></div><p>图4</p>
<blockquote>
<p>如果出现如图的版本信息，那么恭喜你，部署成功了</p>
</blockquote>
<h4 id="拉取模型">
  拉取模型
  <a class="anchor" href="#%e6%8b%89%e5%8f%96%e6%a8%a1%e5%9e%8b">#</a>
</h4>
<p>这一步我们需要拉取需要使用的模型，这里以本次使用的LLama3.1-8B为例。如果使用的是自己的电脑，完全可以自己手动下载模型后移到对应的目录，但如果使用云设备，还是建议使用拉取的方式（省下大量上传时间）</p>
<p>首先，需要安装Hugging Face的指令依赖并登录：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">pip install --upgrade huggingface_hub
huggingface-cli login
</code></pre></div><p>然后拉取需要的模型，' download &lsquo;后更换为自己要下载的模型ID，&rsquo; local-dir ' 则为下载路径</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">huggingface-cli download --resume-download meta-llama/Llama-3.1-8B --local-dir LLaMA-Factory/model
</code></pre></div><blockquote>
<p>Hugging face的使用有一定网络要求，如果你没有设置网络代理的条件，可以考虑设置国内镜像站：https://hf-mirror.com/ 在该网站查看详情；或者直接使用Model Scope的模型拉取服务</p>
</blockquote>
<h4 id="数据集准备">
  数据集准备
  <a class="anchor" href="#%e6%95%b0%e6%8d%ae%e9%9b%86%e5%87%86%e5%a4%87">#</a>
</h4>
<p>首先我们需要把所有准备好的数据集文件移至 LLaMA-Factory/data 目录下，这非常简单，就是简单的移动，没有任何复杂操作。</p>
<p>麻烦的地方在于，我们需要为每个数据集文件“注册”，这样我们才能在LLaMA-Factory中使用它们。</p>
<p>首先需要找到<code>dataset_info.json</code> 文件，它类似于是LLaMA-Factory所有数据集的一个注册表。
用编辑器打开这个文件，然后添加数据集的描述信息。</p>
<p>对于指令微调格式数据集，应该在其中添加如下格式描述：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#e6db74">&#34;数据集名称&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> {  <span style="color:#f92672">&#34;file_name&#34;</span>: <span style="color:#e6db74">&#34;data.json&#34;</span>,  <span style="color:#f92672">&#34;columns&#34;</span>: {    <span style="color:#f92672">&#34;prompt&#34;</span>: <span style="color:#e6db74">&#34;instruction&#34;</span>,    <span style="color:#f92672">&#34;query&#34;</span>: <span style="color:#e6db74">&#34;input&#34;</span>,    <span style="color:#f92672">&#34;response&#34;</span>: <span style="color:#e6db74">&#34;output&#34;</span>,    <span style="color:#f92672">&#34;system&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>,    <span style="color:#f92672">&#34;history&#34;</span>: <span style="color:#e6db74">&#34;history&#34;</span>  }}
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#e6db74">&#34;数据集名称&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> {  <span style="color:#f92672">&#34;file_name&#34;</span>: <span style="color:#e6db74">&#34;data.json&#34;</span>,  <span style="color:#f92672">&#34;columns&#34;</span>: {    <span style="color:#f92672">&#34;prompt&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>  }}
</code></pre></div><blockquote>
<p>不同的数据集格式不同，碍于篇幅，完整格式介绍请前往 
  <a href="https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/data_preparation.html">https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/data_preparation.html</a> 参考</p>
</blockquote>
<blockquote>
<p>如果你总是遇到格式报错，不妨看看示例数据集，找找与你的数据集格式相近的，在 dataset_info.json 复制它们的描述信息，然后在此基础上修改。</p>
<p>另外在文件末尾添加描述时，别忘了最后一段描述是没有逗号的，如果采用复制的方式，有时会需要注意删除逗号（因为中部的每段都有逗号）</p>
</blockquote>
<h4 id="启动web-ui">
  启动Web-UI
  <a class="anchor" href="#%e5%90%af%e5%8a%a8web-ui">#</a>
</h4>
<p>LLaMA-Factory的大部分基本参数都在Web- UI中可调，因此在这里我们主要只使用Web-UI，而不赘述命令行的使用</p>
<p>LLaMA-Factory的前端界面是基于Gradio构建的，ModelScope在云实例中提供了在可直接访问的Localhost（这样你就不用自己去添加域名反代），但是要在这里正常使用Gradio，我们需要对环境变量做一些处理。</p>
<blockquote>
<p>如果你不作处理直接进入，那么很可能会丢失Gradio的前端样式</p>
<p>如果你使用的是自己的设备，则可以完全忽略这一步，直接使用 <code>llamafactory-cli webui</code> 命令启动即可</p>
</blockquote>
<p>如果你碰到了问题，ModelScope给出的解决办法是：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-BASH" data-lang="BASH"><span style="color:#75715e"># dsw-xxx 需要改为具体的实例 id，7860 为 gradio 运行的端口号</span>
export GRADIO_ROOT_PATH<span style="color:#f92672">=</span>/dsw-xxx/proxy/7860/
</code></pre></div><p>或者</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># dsw-xxx 需要改为具体的实例 id，7860 为 gradio 运行的端口号</span>
demo.launch<span style="color:#f92672">(</span>root_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/dsw-xxx/proxy/7860/&#34;</span><span style="color:#f92672">)</span>
</code></pre></div><p>图5</p>
<p>如图，我这里的实例ID为791024，LLaMA-Factory给我的启动端口是7860（其实默认就是7860），那么我就应该在终端输入：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export GRADIO_ROOT_PATH<span style="color:#f92672">=</span>/dsw-791024/proxy/7860/
</code></pre></div><p>然后再使用下面的命令启动Web-UI</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">llamafactory-cli webui
</code></pre></div><p>祝贺你！到了这一步，我们已经完成了部署LLaMA-Factory，可以正式开始准备训练模型了✌️</p>
<h3 id="参数">
  参数
  <a class="anchor" href="#%e5%8f%82%e6%95%b0">#</a>
</h3>
<p>打开Web-UI界面，这里有非常多可以调节的参数</p>
<h4 id="微调方法">
  微调方法
  <a class="anchor" href="#%e5%be%ae%e8%b0%83%e6%96%b9%e6%b3%95">#</a>
</h4>
<p>LoRA、Freeze 和 Full 是三种常见的微调方法，它们各自适用于不同的场景。</p>
<p>LoRA （Low-Rank Adaptation）是一种参数高效微调方法，它通过插入小型可训练的低秩矩阵到模型的权重中，减少了需要更新的参数量。它的特点是：参数量小，仅需训练插入的低秩矩阵，而原模型权重保持冻结；计算资源需求低，显著节省显存和计算量；迁移性强，因为LoRA 参数可以轻松添加到原始模型中，而不需要改变原始模型；训练速度快，因为需要更新的参数少。</p>
<p>Freeze 则是将预训练模型的大部分参数冻结，仅解冻部分层（通常是顶层或特定模块）进行微调。它的参数量中等，相比 LoRA，需要微调的参数更多，但远少于 Full 微调。它可以保留预训练知识，冻结大部分参数避免过度拟合小数据集。此外灵活性强，可以选择解冻不同的层数或模块以适应任务需求。</p>
<p>Full 微调则是对预训练模型的所有参数进行训练更新。因此它参数量大，因为所有参数都参与训练，因此显存占用和计算需求较高；定制化能力强：适合彻底适配一个模型到新任务或领域。但可能丢失泛化能力：如果数据量不足，容易出现过拟合。</p>
<p>我们针对自身状况来考虑🤔：（仅对于我们最开始提交的模型v1.1而言）</p>
<ol>
<li>
<p>时间和算力资源都非常稀缺，数据集的量也不够庞大</p>
</li>
<li>
<p>不用大幅改变原模型的能力，只需改变它的角色立场和语言风格，以及添加部分知识</p>
</li>
<li>
<p>我们不希望训练出来的Trump是个傻子，只会按数据集说话，应当保留原模型的常规知识和一定泛化能力</p>
</li>
</ol>
<p>综合来看，Lora是最适合我们的选择</p>
<h4 id="量化">
  量化
  <a class="anchor" href="#%e9%87%8f%e5%8c%96">#</a>
</h4>
<p>LLaMA-Factory支持量化训练的功能（QLora），none、8 和 4 代表不同的量化等级，主要是指量化过程中模型参数精度的降低程度。目的是减小模型的大小，减少计算资源的消耗，并提高推理速度。不同的量化等级有不同的精度和性能权。</p>
<p>None指不启用量化，模型保持原始的浮动精度（通常是32位浮动点数）。也就是没有任何精度损失</p>
<p>8 bit量化会将模型的权重和激活函数转换为8位整数（通常是 int8），能够显著减少内存占用（我们测试大约原始模型的1/4）</p>
<p>4 bit量化会将模型的权重和激活函数转换为4位整数（通常是 int4）。能够进一步减少内存占用（我们测试约原始模型的1/8）</p>
<p>如果资源足够，肯定选择None不量化啊😭。但是我们发现如果不量化，时间成本将变的难以接受（其实只是赶不上DDL）。随后我们又测试其他两种量化规格，发现4bit量化训练虽然快，但效果十分不理想。而8bit量化的模型回答质量基本没有明显下降。</p>
<p>所以我们选择None + 8bit结合的方式，大部分数据集都采用8bit量化训练，有一些比较小的数据集（比如identity），就不进行量化。</p>
<blockquote>
<p>ps：目前已经没有这个烦恼了，感谢吴晓晓老师提供的4090和A100，我们仍在持续更新模型中&hellip;</p>
</blockquote>
<h4 id="量化方法">
  量化方法
  <a class="anchor" href="#%e9%87%8f%e5%8c%96%e6%96%b9%e6%b3%95">#</a>
</h4>
<p>Bitsandbytes、hqq 和 eetq，这仨在量化的策略、实现和适用场景上有所不同。</p>
<p>BitsAndBytes 是Hugging Face的一个开源库，用于在不牺牲太多性能的情况下执行高效的量化，特别是针对大规模预训练模型（如 GPT、BERT 等）的量化。它特别擅长通过优化的低秩近似和稀疏技术来提高量化效率。主要方式是“后训练量化”（post-training quantization），适合用于已经训练好的模型。也就是说很适合用于在已有模型上进行微调的场景。</p>
<p>HQQ (High-Quality Quantization)是一种高质量量化方法，重点在于通过先进的数学和统计技术来减少量化对模型性能的影响。它通过更加精细的策略来量化模型的权重。关注高精度量化的细节，力求在压缩模型的同时尽量减少精度损失。
该方法通常使用更复杂的量化算法，如基于误差的量化（error feedback），并且优化量化过程中的梯度传播。
它比较适合对精度要求较高的推理任务，比如需要对话理解、机器翻译等复杂任务。当模型压缩后需要保持较高精度，尤其在数据量较小或者推理任务精度要求较高时。</p>
<p>EETQ (Efficient Error-Tracing Quantization)是一种专门用于量化大模型的高效方法，它的核心思想是通过对量化过程中的误差传播进行优化，调整模型权重的量化策略。然后通过实时计算误差并对其进行调整，从而减少因量化引起的性能损失。</p>
<p>它特别适合复杂模型和需要在高精度和低资源之间找到平衡的场景。在处理非常大的模型时特别有效，能够保持精度的同时降低计算和内存消耗。当需要处理大规模模型且不能接受过多精度损失时，这是个不错的选择。</p>
<p>我们训练的模型并不是用来处理专业的特定任务，也能承受一定的精度损失（把话说明白就好，不用推理专业问题或者算数），此外尺寸也不大（8B），再加上Bitsandbytes由Hugging Face支持，相关文档的社区资源都更丰富，出了问题好解决👌，因此权衡之下选择Bitsandbytes方法。</p>
<h4 id="提示模板">
  提示模板
  <a class="anchor" href="#%e6%8f%90%e7%a4%ba%e6%a8%a1%e6%9d%bf">#</a>
</h4>
<p>每个大模型在构建时都需要一个提示模版(Prompt Template)，它本质上是一种结构化的框架，用来规范和优化与大语言模型的交互方式。就像我们在和人交谈时会遵循某些社交礼仪和对话模式一样，提示模版帮助我们以更有效的方式与AI模型进行沟通。如果你的提示模版不正确，那么模型可能会出现变蠢🤤、重复循环🔁、胡言乱语🤪等现象</p>
<p>我们在训练时选择了默认模版 Default</p>
<blockquote>
<p>提示模版是一个大坑</p>
<p>在你不是很熟悉提示模版的情况下，优先在里面查找你要训练的模型的同名模版。比如你现在训练的是qwen 2.5 7B模型，那么就选择“qwen”这个模版</p>
<p>此外有一些模型在不同的版本会使用不一样的提示模版，比如LLama 3.1模型就使用了和LLama 3不一样的提示模版。因此当你训练LLama 3.1模型时，最好就不要选择“LLama 3”这个提示模版</p>
<p>如果可选模版里没有你当前正在使用的模型，选择“Default”是可行的</p>
<p>如果你打算自己写，请一定要去模型官方的库查看官方给出的提示模版</p>
</blockquote>
<h4 id="rope插值">
  RoPE插值
  <a class="anchor" href="#rope%e6%8f%92%e5%80%bc">#</a>
</h4>
<p>RoPE （Rotary Positional Encoding）是一种用于处理序列中位置信息的技术，它为模型提供了序列中每个元素的位置编码，帮助模型理解输入数据的顺序。而none、linear 和 dynamic 是三种不同的插值方法，它们影响的是 位置编码 在序列长度变化时如何插值或扩展。在处理不同长度的输入时，如何调整这些位置编码非常重要。</p>
<p>None（不进行插值）意味着不会对位置编码进行任何插值处理。如果模型的输入长度超出了预先设定的最大序列长度，位置编码将不会扩展，而是保持原样。而由于没有插值，因此对于超出最大长度的输入，位置编码可能无法准确表示。可能导致模型在处理长序列时的表现下降。它比较适合已经固定了最大序列长度，且输入长度较为稳定的任务，或者当输入序列的长度不会超出模型最大长度时使用。</p>
<p>Linear（线性插值）根据输入序列的长度线性地扩展或调整位置编码。即，如果输入的序列长度超过预定的最大长度，位置编码会按线性关系进行延伸，直到覆盖整个输入序列。通过线性插值，位置编码在超出最大长度时仍然可以保持合理的连续性。这种方法能平滑地扩展位置编码，但由于插值是线性的，可能导致模型在某些任务上的表现不如动态插值。它适用于输入序列长度变化不大，但仍需要一定扩展性和灵活性的场景。比如那些不要求非常精确位置编码的任务。</p>
<p>Dynamic（动态插值）根据具体输入的长度和上下文，灵活地调整位置编码。与线性插值不同，动态插值能够基于输入的具体需求动态调整位置编码，考虑到输入序列的实际分布和结构。它可以更精确地调整位置编码，使得模型对不同输入序列的长度变化适应得更好。它通常会利用额外的上下文信息来优化编码效果，适用于输入长度变化范围较大，且需要精确位置编码的任务。特别是长文本处理或需要精细序列建模的任务。但问题是计算相对复杂，可能会增加模型的训练时间和计算开销。</p>
<p>我们的指令微调数据大多为采访，没有很长的序列，因此可以直接选择None</p>
<p>而预训练数据集有些演讲的文段超出了最大序列，正常来说应该使用Dynamic动态差值，但一是时间成本不够，而是超出最大序列的片段数量不是特别多，因此我们在数据集中手动切分了那些一眼看上去都明显偏长的文段，</p>
<p>并最后统一选择None不差值</p>
<h4 id="训练方式">
  训练方式
  <a class="anchor" href="#%e8%ae%ad%e7%bb%83%e6%96%b9%e5%bc%8f">#</a>
</h4>
<p>在训练模型的过程中，选择不同的训练方式通常取决于模型的目标任务、数据类型以及你希望模型具备的能力。</p>
<p>Supervised Fine-Tuning（监督微调）是在一个已经预训练的模型基础上，使用标签明确的数据（例如有标签的输入-输出对）对模型进行进一步训练。这种方法利用现有的标注数据，通过最小化损失函数来调整模型的权重。</p>
<p>Reward Modeling（奖励建模）是一种基于强化学习的训练方式，通常用于生成模型或决策模型。通过定义一个奖励函数，模型根据任务的目标来优化输出，使得产生的结果更符合预期目标。该方法通常用于训练能够生成文本或进行复杂决策的模型。</p>
<p>PPO (Proximal Policy Optimization)是一种强化学习算法，主要用于训练智能体（Agent）进行决策。它通过不断试验和调整模型行为，使其通过奖励信号优化行为策略。是基于策略的强化学习方法，常用于解决强化学习中的策略优化问题。</p>
<p>DPO (Direct Preference Optimization)是一种用于优化生成模型的算法，它通过直接优化用户的偏好来改进模型输出的质量。与传统的强化学习方法不同，它不依赖于传统的奖励信号，而是通过直接调整模型对用户偏好的适应性来优化结果。适用于对话生成、推荐系统等任务，特别是在生成任务中对用户偏好有强烈需求的场景。（比如客服💁）</p>
<p>KTO (Knowledge Tracing Optimization) 是一种优化学习模型的方法，特别针对教育技术和智能辅导系统。它的核心目标是通过学习用户的学习轨迹和知识掌握情况来优化模型。KTO 方法通过评估学生的学习情况，动态调整学习策略，帮助个性化推荐和教学。适用于教育技术、智能辅导、学习评估等领域，帮助系统根据学生的知识掌握情况进行个性化推荐和优化。（Google的Learn LLM和Claude的Explanatory模式）</p>
<p>Pre-Training（预训练）是一种基础的训练方法，模型首先在大规模数据集上进行训练，通常是无监督学习阶段。预训练阶段的目标是学习一般的知识和模式，模型学习到的知识可以被迁移到下游任务中（如微调阶段）。适用于大规模预训练模型，尤其是自然语言处理任务，如 BERT、GPT、T5 等。预训练使模型能够学习到通用的语言模式和结构，之后可以在特定任务中进行微调。</p>
<p>结合我们这个项目的目标🎯和数据集资源情况，这里只需根据数据集类型选择训练方式即可。</p>
<p><strong>指令微调数据集</strong>（identity、采访等）选择Supervised Fine-Tuning（监督微调）</p>
<p><strong>预训练数据集</strong>（推文，演讲等）选择Pre-Training（预训练）</p>
<h4 id="学习率">
  学习率
  <a class="anchor" href="#%e5%ad%a6%e4%b9%a0%e7%8e%87">#</a>
</h4>
<p>学习率（Learning Rate）是深度学习中一个至关重要的超参数，它决定了模型在训练过程中每次更新权重时的步伐大小。简单来说，学习率控制了优化算法（如 AdamW）在梯度下降过程中更新权重的速度。选择合适的学习率对于模型的训练效果和速度都有重大影响。</p>
<p><strong>学习率的作用</strong></p>
<p>在训练神经网络时，优化器使用梯度下降算法来最小化损失函数，通过更新模型的参数（如权重和偏置）。每次参数更新的幅度由学习率来控制。具体来说，学习率决定了每次迭代时，参数沿着负梯度的方向更新的步长：</p>
<p>$                           \theta = \theta - \eta \cdot \nabla_{\theta} J(\theta)​$</p>
<p>其中：
$ \theta $ 是模型的参数（如权重）。
$ \eta $ 是学习率（Learning Rate）。
$ \nabla_{\theta} J(\theta) $ 是损失函数 $ J(\theta) $ 相对于参数 $ \theta $ 的梯度。</p>
<p><strong>AdamW 优化器与学习率</strong></p>
<p>AdamW 是一种广泛使用的优化器，它是 Adam 优化器的改进版本，具有权重衰减（Weight Decay）机制，适用于很多深度学习模型。</p>
<p>AdamW 的特点是结合了动量和自适应学习率调整，它在训练时会根据历史梯度调整每个参数的学习率，从而加速收敛并避免震荡。</p>
<p>选择合适的学习率是训练深度学习模型的关键。如果学习率过大，模型的参数更新步伐过大，可能导致训练过程不稳定，甚至无法收敛（梯度爆炸）。而如果学习率过小，训练过程可能会过慢，甚至在达不到全局最优解之前就停滞了。如果学习率设置得过大，可能导致训练过程中损失值波动较大，甚至不收敛。如果学习率过小，训练过程将非常缓慢，可能在长时间内看不到明显的进展。而在训练过程中逐渐减少学习率是一个常见的做法。一般来说，训练初期使用较大的学习率，随着训练的进行逐渐减小，以便模型能够细致地调整权重，达到更好的收敛效果。
在训练过程中，有一些常用的调整策略：</p>
<ul>
<li>
<p>手动调整：根据训练的进展调整学习率，通常在损失曲线变得平稳时，降低学习率，以便进行精细化调整。</p>
</li>
<li>
<p>学习率预热（Learning Rate Warmup）：在训练的前几个 epochs 中，使用较小的学习率，并逐步增加到目标学习率。这样可以避免初期权重更新过快，导致模型发散。</p>
</li>
<li>
<p>学习率衰减（Learning Rate Scheduling）：使用逐步衰减的方法（例如指数衰减、余弦衰减）或基于性能的调整策略（例如，当验证损失不再下降时减少学习率）。</p>
</li>
</ul>
<blockquote>
<p>我们要根据实际情况（比如损失曲线）动态调整学习率</p>
</blockquote>
<p>如果没空一直盯着👀，或者不会判断，或者单纯的懒怎么办？</p>
<p>那么建议直接使用<strong>5e-5</strong></p>
<p>一个非常常见的默认值，尤其是在自然语言处理任务中（例如训练大规模的预训练语言模型）。这个学习率适合大多数情况，因为它通常能够在训练过程中平稳地更新模型权重，避免过快或过慢的更新。（我们就是这样做的，但不是因为懒，是因为没时间，真的😭）</p>
<blockquote>
<p>什么？你说你不懂5e-5，1e-5这些数字的含义？</p>
<p>这样的写法实际上是科学计数法的简写。</p>
<p>让我们把它拆解来看：</p>
<ul>
<li>
<p>5e-5 表示 5 × 10⁻⁵</p>
</li>
<li>
<p>1e-5 表示 1 × 10⁻⁵</p>
</li>
</ul>
<p>直观来说，e-5 就是&quot;小数点向左移动5位&quot;。以下是一个参考：</p>
<p>学习率从大到小排列</p>
<p>1e-3  = 0.001      = 1/1000</p>
<p>5e-4  = 0.0005     = 1/2000</p>
<p>1e-4  = 0.0001     = 1/10000</p>
<p>5e-5  = 0.00005    = 1/20000</p>
<p>1e-5  = 0.00001    = 1/100000</p>
<p>5e-6  = 0.000005   = 1/200000</p>
<p>1e-6  = 0.000001   = 1/1000000</p>
</blockquote>
<h4 id="训练轮数">
  训练轮数
  <a class="anchor" href="#%e8%ae%ad%e7%bb%83%e8%bd%ae%e6%95%b0">#</a>
</h4>
<p>训练轮数（Epochs） 表示整个训练集通过模型的次数。在每个训练轮次（Epoch）中，模型会遍历一次训练数据集，并根据计算出的梯度更新模型的权重。训练轮数的选择直接影响模型的最终表现和训练时间。</p>
<p>调整训练轮数主要参考下面几个因素：</p>
<ol>
<li>数据量与任务复杂度</li>
</ol>
<p>如果数据集非常大，通常需要更多的训练轮数才能充分训练模型。此时，较少的轮次（比如默认的3 轮）可能不足以让模型收敛，可能需要增加训练轮数，以使模型能够充分学习到数据中的模式。如果任务较为复杂（例如文本生成、对话系统），可能需要更多的训练轮数，以确保模型能够学习到足够的细节。</p>
<ol start="2">
<li>模型是否预训练</li>
</ol>
<p>如果使用的是一个已经预训练过的模型（例如 GPT、BERT），通常 默认3 轮微调都已经已经足够，因为模型已经学到了很多通用的知识。但如果是在没有任何预训练的情况下训练模型，通常需要更多的训练轮数，比如几十轮训练，才能得到合适的结果。</p>
<ol start="3">
<li>模型的验证表现</li>
</ol>
<p>在训练过程中，使用验证集监控模型的表现非常重要。如果验证集的性能在训练过程中已经达到一个稳定的水平，继续增加训练轮数可能不会带来太多的提升，甚至可能导致过拟合（模型可能已经收敛）。我们可以使用早停策略（Early Stopping），即在验证集上的性能没有显著提升时（或者损失率不下降）提前停止训练。</p>
<ol start="4">
<li>训练时间和资源</li>
</ol>
<p>如果训练时间和计算资源有限，可以选择较少的轮数，并结合其他方法（例如使用较高的学习率、批量大小等）来加速训练。如果计算资源丰富，并且希望模型能达到更高的精度，可以选择增加训练轮数，或者结合更复杂的训练策略来优化性能。</p>
<p>虽然我们是在已有模型（LLama3.1）的基础上微调，但我们主要让它学习新的语言模式，所以训练轮次不能太低，因此我们的策略是默认每个数据集都训练20轮，有少数数据集因为时间资源的原因，不得不减少至10轮。</p>
<p>以上基本就是正式训练前需要调整的参数，其他的参数如果没有特殊需求维持默认即可。</p>
<h3 id="训练过程">
  训练过程
  <a class="anchor" href="#%e8%ae%ad%e7%bb%83%e8%bf%87%e7%a8%8b">#</a>
</h3>
<h4 id="开始">
  开始
  <a class="anchor" href="#%e5%bc%80%e5%a7%8b">#</a>
</h4>
<p>在设置好参数后，首先我们要选择数据集。如果已经将数据集文件移至正确的目录并在 dataset_info.json 中标注完成，那么你应该能直接在数据集列表中选择。</p>
<p>在选择完之后，最好预览一下数据集，检查是否有问题。</p>
<p>设置妥当之后，点击预览命令来检查参数是否全部设置正确。</p>
<blockquote>
<p>每个训练命令都可以按照这个格式写成一份yaml文件, 然后在终端中使用 <code>llamafactory-cli train example.yaml</code>命令来进行</p>
</blockquote>
<p>点击开始，就可以开始训练数据集了</p>
<p>图片6</p>
<h4 id="中断">
  中断
  <a class="anchor" href="#%e4%b8%ad%e6%96%ad">#</a>
</h4>
<p>有时我们可能会因为某些情况不得不终止当前的训练（比如达到实例单次最大时长，或者用自己电脑时宵禁断电）。这时我们需要点击“中断”来停止此次训练，然后“保存训练参数”</p>
<p>训练参数会以yaml格式保存在 <code>LLaMA-Factory/saves</code> 目录下，当你下次开机想要继续训练时，只需在配置路径中选择上次的训练参数，然后点击“载入训练参数”，用“预览命令”确认一下之后再次开始就行。</p>
<h4 id="检查点">
  检查点
  <a class="anchor" href="#%e6%a3%80%e6%9f%a5%e7%82%b9">#</a>
</h4>
<p>我们不可能一次性训练完所有的数据集，因此检查点（Checkpoint）是不可或缺的，在我们给模型练完一个数据集之后，下一次训练时该从哪里开始？</p>
<p>在没有合并导出之前，模型并不会“记得”上一次的训练，所以需要检查点来存储我们的阶段性成果，告诉程序该从这个状态的基础上接着训练</p>
<p>图片7</p>
<blockquote>
<p>比如我在2024-12-17号完成了数据集1的训练，那么当我在2024-12-18号训练数据集2时，就应选择2024-12-17这个检查点，然后在下一次训练数据集3时选择2024-12-18号的检查点</p>
</blockquote>
<h4 id="损失率">
  损失率
  <a class="anchor" href="#%e6%8d%9f%e5%a4%b1%e7%8e%87">#</a>
</h4>
<p>在Web-UI里，会显示当前训练的损失率（和纯命令行相比的又一个优点）</p>
<p>图片8</p>
<p>这图是什么意思？</p>
<p>横轴（x轴）：标示的是训练的步数（step），也就是模型训练的迭代次数。每一步表示模型通过一个或一批数据样本完成了前向传播、反向传播以及参数更新。</p>
<p>纵轴（y轴）：标示的是损失值（loss），用于衡量模型预测结果与真实标签之间的偏差。较高的损失值表示模型表现较差，而较低的损失值意味着模型的预测更接近真实值。</p>
<p>浅蓝色曲线（original）：表示原始损失值，未经平滑处理。由于训练过程中数据分布和学习率的波动，原始曲线可能会出现较大的起伏。</p>
<p>深蓝色曲线（smoothed）：表示平滑后的损失曲线，目的是通过平滑算法（例如移动平均）来减少噪声，显示出损失的整体趋势，便于观察模型的学习效果。</p>
<p>这个损失函数实际上是衡量模型预测结果与真实值之间差距的指标。就像考试打分一样，只不过是分数越低说明答得越好。在语言模型中，这个损失通常用交叉熵(Cross Entropy)来计算，它反映了模型在预测下一个词时的准确程度。</p>
<p>想象数据集就是一个篮筐，数据集的内容是确定的，就好比篮筐的位置是固定的；模型一开始的回答可能会和数据集差很多，就好比投篮投的偏离了许多；我们训练的目标是让这个距离（损失值）尽可能小，从而提高投篮的准确度（模型的性能）。而我们把每一次偏离的具体结果记录下来，连成线，就得到了原始损失曲线（淡蓝色那个）；但为了便于分析整体趋势，我们要对原始损失值进行平滑处理，通常使用移动平均（Moving Average）来计算，然后得到平滑曲线（深蓝色那个）</p>
<p>👀那么我们该怎么看这平滑曲线呢？</p>
<ol>
<li>
<p>初始损失值：这个值反映了模型的初始状态。对于给定的任务，通常有一个合理的初始损失范围，超出这个范围可能意味着模型初始化有问题。</p>
</li>
<li>
<p>下降速度（学习率的表现）： 就像学习新知识一样，损失下降速度反映了模型的学习效率：太快的下降可能意味着学习率过大，模型可能会错过最优解；太慢的下降说明学习率可能过小，训练效率不高。理想的情况是开始时快速下降，然后逐渐平缓</p>
</li>
<li>
<p>波动程度（稳定性）： 观察原始曲线（浅蓝色）的波动，适度的波动是正常的，反映了不同批次数据的差异，但过大的波动可能暗示学习率过高或batch size过小；突然的剧烈波动则可能意味着出现了梯度爆炸</p>
</li>
<li>
<p>收敛特征： 在训练后期，我们要观察损失是否真正稳定：如果曲线完全平坦，可能陷入了局部最优，如果仍在缓慢下降，可能还有优化空间，理想的情况是在一个小范围内轻微波动</p>
</li>
</ol>
<p>综合看：</p>
<ul>
<li>
<p>最终损失值是否达到预期范围</p>
</li>
<li>
<p>收敛过程是否平稳</p>
</li>
<li>
<p>训练是否充分（步数够不够）</p>
</li>
<li>
<p>有没有异常的波动或震荡</p>
</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">

  



  
    
  
    
  


  


<div class="book-languages" tabindex="0" aria-haspopup="true">
  <ul>
    <li class="flex align-center">
      <img src="/svg/translate.svg" class="book-icon" alt="Languages" />
      Chinese
    </li> 
  </ul>

  <ul class="book-languages-list">
    
    <li class="active">
      <a href="https://eesissi.com/zh/" class="flex align-center">
        <img src="/svg/translate.svg" class="book-icon" alt="Languages" />
        Chinese
      </a>
    </li>
    
    <li class="">
      <a href="https://eesissi.com/docs/example/course1/" class="flex align-center">
        <img src="/svg/translate.svg" class="book-icon" alt="Languages" />
        English
      </a>
    </li>
    
  </ul>
</div>






  <div>
    <a class="flex align-center" href="https://github.com/alex-shpak/hugo-book/edit/master/exampleSite/content.zh/docs/example/course1.md" target="_blank" rel="noopener">
      <img src="/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>编辑本页</span>
    </a>
  </div>

</div>

 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#artificial-intelligence-and-machine-learning">Artificial Intelligence and Machine Learning</a>
      <ul>
        <li><a href="#人工智能与机器学习">人工智能与机器学习</a>
          <ul>
            <li><a href="#课题背景">课题背景</a></li>
          </ul>
        </li>
        <li><a href="#数据集收集清洗与训练">数据集收集，清洗与训练</a>
          <ul>
            <li><a href="#数据来源">数据来源</a></li>
            <li><a href="#预训练数据集">预训练数据集</a></li>
            <li><a href="#指令微调数据集">指令微调数据集</a></li>
          </ul>
        </li>
        <li><a href="#模型训练">模型训练</a>
          <ul>
            <li><a href="#工具依赖和前置准备">工具、依赖和前置准备</a></li>
            <li><a href="#模型选择">模型选择</a></li>
            <li><a href="#部署与配置">部署与配置</a></li>
            <li><a href="#参数">参数</a></li>
            <li><a href="#训练过程">训练过程</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>

</html>












