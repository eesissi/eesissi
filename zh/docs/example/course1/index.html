<!DOCTYPE html>
<html lang="zh" dir="ltr">

<head>
  <meta name="generator" content="Hugo 0.92.0" />
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Artificial Intelligence and Machine Learning#人工智能与机器学习#课题名称：LLama Trump
作者：2022级金融科技专业 傅楠迪 &amp; 祝华卿
课题背景#在历史的长河中，众多伟人以其独特的影响力和鲜明的个人特质被人们铭记于心，而当我们谈及说话极具特点的伟人时，Donald Trump（唐纳德・特朗普）的形象往往会迅速跃然于许多人的脑海之中。他那极具辨识度的语言风格，无论是简洁有力的表达，还是独树一帜的用词习惯，都给世人留下了深刻的印象。
恰逢特朗普当选为美国第 47 任总统这一重要历史节点，我们团队从中捕捉到了独特的项目灵感。怀揣着探索创新与技术应用的热情，我们期望借助先进的技术手段，去复现一个能够展现特朗普说话风格的大模型。于是，经过近一个月的不懈努力，我们团队成员们不断钻研、反复调试，于是LLama Trump 这个凝聚着大家心血与智慧的大模型应运而生。
项目目标：训练一个具有特朗普语言特色的角色模型。
数据集收集，清洗与训练#数据来源#当前版本模型所使用的数据集原资料来源主要为：
  www.presidency.ucsb.edu
  GitHub项目Trump_Campaign_Corpus
  合成数据
  纽约时报
  http://www.presidency.ucsb.edu 是美国总统档案项目（American Presidency Project, APP）的官方网站。该网站是一个非营利性和非党派的资源，专注于提供与美国总统相关的文献和资料。
图片1
GitHub项目 Trump_Campaign_Corpus 包括特朗普的演讲、访谈、辩论、市政厅会议、新闻发布会、书面声明和推文。涵盖了从 2015 年 6 月 16 日宣布竞选到 2016 年 11 月 8 日选举日的时期。是初步格式化数据，可在版权允许的情况下使用。">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="" />
<meta property="og:description" content="Artificial Intelligence and Machine Learning#人工智能与机器学习#课题名称：LLama Trump
作者：2022级金融科技专业 傅楠迪 &amp; 祝华卿
课题背景#在历史的长河中，众多伟人以其独特的影响力和鲜明的个人特质被人们铭记于心，而当我们谈及说话极具特点的伟人时，Donald Trump（唐纳德・特朗普）的形象往往会迅速跃然于许多人的脑海之中。他那极具辨识度的语言风格，无论是简洁有力的表达，还是独树一帜的用词习惯，都给世人留下了深刻的印象。
恰逢特朗普当选为美国第 47 任总统这一重要历史节点，我们团队从中捕捉到了独特的项目灵感。怀揣着探索创新与技术应用的热情，我们期望借助先进的技术手段，去复现一个能够展现特朗普说话风格的大模型。于是，经过近一个月的不懈努力，我们团队成员们不断钻研、反复调试，于是LLama Trump 这个凝聚着大家心血与智慧的大模型应运而生。
项目目标：训练一个具有特朗普语言特色的角色模型。
数据集收集，清洗与训练#数据来源#当前版本模型所使用的数据集原资料来源主要为：
  www.presidency.ucsb.edu
  GitHub项目Trump_Campaign_Corpus
  合成数据
  纽约时报
  http://www.presidency.ucsb.edu 是美国总统档案项目（American Presidency Project, APP）的官方网站。该网站是一个非营利性和非党派的资源，专注于提供与美国总统相关的文献和资料。
图片1
GitHub项目 Trump_Campaign_Corpus 包括特朗普的演讲、访谈、辩论、市政厅会议、新闻发布会、书面声明和推文。涵盖了从 2015 年 6 月 16 日宣布竞选到 2016 年 11 月 8 日选举日的时期。是初步格式化数据，可在版权允许的情况下使用。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://eesissi.com/zh/docs/example/course1/" /><meta property="article:section" content="docs" />



<title>Course1 | Xiaoxiao Wu</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="alternate" hreflang="en" href="https://eesissi.com/docs/example/course1/" title="Course1">

<link rel="stylesheet" href="/book.min.6c7c6446dfdee7c8c933e9bbc6e80ee3ed6c913b2a59519f2092c3c6a9d63e55.css" integrity="sha256-bHxkRt/e58jJM&#43;m7xugO4&#43;1skTsqWVGfIJLDxqnWPlU=">
<script defer src="/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js" integrity="sha256-b2&#43;Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC&#43;NdcPIvZhzk="></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a href="/zh"><span>Xiaoxiao Wu</span>
  </a>
</h2>












  <ul>
<li>
  <a href="/zh/"><strong>Introduction</strong></a></li>
<li>
  <a href="/zh/docs/example/publication/"><strong>Publication</strong></a></li>
<li>
  <a href="/zh/docs/example/project/"><strong>Project</strong></a></li>
<li>
  <a href="/zh/docs/example/members/"><strong>Group Members</strong></a></li>
<li>
  <a href="/zh/docs/example/"><strong>Gallery</strong></a></li>
<li>
  <a href=""><strong>Current Courses</strong></a>
<ul>
<li>
  <a href="/zh/docs/example/course1/"class=active><strong>Artificial Intelligence and Machine Learning</strong></a></li>
<li>
  <a href="/zh/docs/example/course2/"><strong>Data Mining</strong></a></li>
</ul>
</li>
</ul>










</nav>




  <script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Course1</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#artificial-intelligence-and-machine-learning">Artificial Intelligence and Machine Learning</a>
      <ul>
        <li><a href="#人工智能与机器学习">人工智能与机器学习</a>
          <ul>
            <li><a href="#课题背景">课题背景</a></li>
          </ul>
        </li>
        <li><a href="#数据集收集清洗与训练">数据集收集，清洗与训练</a>
          <ul>
            <li><a href="#数据来源">数据来源</a></li>
            <li><a href="#预训练数据集">预训练数据集</a></li>
            <li><a href="#指令微调数据集">指令微调数据集</a></li>
          </ul>
        </li>
        <li><a href="#模型训练">模型训练</a>
          <ul>
            <li><a href="#工具依赖和前置准备">工具、依赖和前置准备</a></li>
            <li><a href="#模型选择">模型选择</a></li>
            <li><a href="#部署与配置">部署与配置</a></li>
            <li><a href="#参数">参数</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="artificial-intelligence-and-machine-learning">
  Artificial Intelligence and Machine Learning
  <a class="anchor" href="#artificial-intelligence-and-machine-learning">#</a>
</h1>
<h2 id="人工智能与机器学习">
  人工智能与机器学习
  <a class="anchor" href="#%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e4%b8%8e%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0">#</a>
</h2>
<p><strong>课题名称</strong>：LLama Trump</p>
<p><strong>作者</strong>：2022级金融科技专业 傅楠迪 &amp; 祝华卿</p>
<h3 id="课题背景">
  课题背景
  <a class="anchor" href="#%e8%af%be%e9%a2%98%e8%83%8c%e6%99%af">#</a>
</h3>
<p>在历史的长河中，众多伟人以其独特的影响力和鲜明的个人特质被人们铭记于心，而当我们谈及说话极具特点的伟人时，Donald Trump（唐纳德・特朗普）的形象往往会迅速跃然于许多人的脑海之中。他那极具辨识度的语言风格，无论是简洁有力的表达，还是独树一帜的用词习惯，都给世人留下了深刻的印象。</p>
<p>恰逢特朗普当选为美国第 47 任总统这一重要历史节点，我们团队从中捕捉到了独特的项目灵感。怀揣着探索创新与技术应用的热情，我们期望借助先进的技术手段，去复现一个能够展现特朗普说话风格的大模型。于是，经过近一个月的不懈努力，我们团队成员们不断钻研、反复调试，于是LLama Trump 这个凝聚着大家心血与智慧的大模型应运而生。</p>
<p><strong>项目目标</strong>：训练一个具有特朗普语言特色的角色模型。</p>
<h2 id="数据集收集清洗与训练">
  数据集收集，清洗与训练
  <a class="anchor" href="#%e6%95%b0%e6%8d%ae%e9%9b%86%e6%94%b6%e9%9b%86%e6%b8%85%e6%b4%97%e4%b8%8e%e8%ae%ad%e7%bb%83">#</a>
</h2>
<h3 id="数据来源">
  数据来源
  <a class="anchor" href="#%e6%95%b0%e6%8d%ae%e6%9d%a5%e6%ba%90">#</a>
</h3>
<p>当前版本模型所使用的数据集原资料来源主要为：</p>
<ul>
<li>
<p>
  <a href="http://www.presidency.ucsb.edu">www.presidency.ucsb.edu</a></p>
</li>
<li>
<p>GitHub项目Trump_Campaign_Corpus</p>
</li>
<li>
<p>合成数据</p>
</li>
<li>
<p>纽约时报</p>
</li>
</ul>
<p>
  <a href="http://www.presidency.ucsb.edu">http://www.presidency.ucsb.edu</a> 是美国总统档案项目（American Presidency Project, APP）的官方网站。该网站是一个非营利性和非党派的资源，专注于提供与美国总统相关的文献和资料。</p>
<p>图片1</p>
<p>GitHub项目 Trump_Campaign_Corpus 包括特朗普的演讲、访谈、辩论、市政厅会议、新闻发布会、书面声明和推文。涵盖了从 2015 年 6 月 16 日宣布竞选到 2016 年 11 月 8 日选举日的时期。是初步格式化数据，可在版权允许的情况下使用。</p>
<p>图片2</p>
<blockquote>
<p>git地址：https://github.com/unendin/Trump_Campaign_Corpus.git</p>
</blockquote>
<p>针对一些模型认识性的问题，我们则人工制作了一些合成数据集</p>
<h3 id="预训练数据集">
  预训练数据集
  <a class="anchor" href="#%e9%a2%84%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae%e9%9b%86">#</a>
</h3>
<h4 id="speech">
  Speech
  <a class="anchor" href="#speech">#</a>
</h4>
<p>演讲资料的来源主要为 Trump_Campaign_Corpus 和 
  <a href="http://www.presidency.ucsb.edu">www.presidency.ucsb.edu</a> 。针对 Trump_Campaign_Corpus 的初步格式化数据，我们自己设计制作了二次格式化程序，将其制作为符合模型训练格式的数据集；并用爬虫获取 
  <a href="http://www.presidency.ucsb.edu">www.presidency.ucsb.edu</a> 中特朗普在第一任期的演讲内容。</p>
<p>对于 Trump_Campaign_Corpus 的初步格式化数据，我们在基本的指令微调格式上添加了多轮对话。</p>
<p>处理之后，对于原文件：</p>
<pre tabindex="0"><code>AUDIENCE: (Applause)
DONALD TRUMP: Thank you all for being here today.
We are going to make America great again.
(AUDIENCE CHEERS)
DONALD TRUMP: We will focus on creating jobs and reducing taxes.
</code></pre><p>处理后得到：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">[
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;Thank you all for being here today. We are going to make America great again.&#34;</span>
    },
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;We will focus on creating jobs and reducing taxes.&#34;</span>
    }
]
</code></pre></div><p>而对于www.presidency.ucsb.edu，我们总共爬取了991个演讲数据，但由于原文件本身的格式特别混乱，因此清洗难度比较大，因此在清洗和制作数据集的过程中舍弃掉了许多无力处理的文件，最终筛选制作了314个演讲数据集。</p>
<p>之后我们针对这种类型的演讲制作了一个清洗程序，将爬取的文件清洗为符合模型训练格式的数据集。</p>
<p>最后，我们将所有清洗完的演讲合并到一个json文件里，格式如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-JSON" data-lang="JSON">[
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;Beautiful. So, so nice.   So nice. So they say we set a record tonight.   Now, I am going to make the people outside angry, but they love me and I love them. We have over 1,000 people outside...&#34;</span>
    },
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;Wow. Wow, wow, wow! Unbelievable! Unbelievable!  Thank you.  Oh, that&#39;s so beautiful...&#34;</span>
    }
]
</code></pre></div><h4 id="tweets">
  Tweets
  <a class="anchor" href="#tweets">#</a>
</h4>
<p>我们编写爬虫从www.presidency.ucsb.edu获取Trump第一任期的推文信息。</p>
<blockquote>
<p>由于www.presidency.ucsb.edu是境外网站，所以在爬取过程中使用了代理</p>
</blockquote>
<p>我们一共爬去了<strong>2035</strong>个推特界面，经过数据清洗后，按照预训练数据集的格式，合并为一个文件，最终获得包含<strong>15293</strong>条有效推文的数据集文件，以下是该文件的部分样例</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-JSON" data-lang="JSON">[
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;President Donald J. Trump and Melania Participate in the Pardoning of the National Thanksgiving Turkey at the White House.&#34;</span>
    },
    {
        <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;Today we continued a wonderful American Tradition at the White House. Drumstick and Wishbone will live out their days in the beautiful Blue Ridge Mountains at Gobblers Rest...&#34;</span>
    }
]
</code></pre></div><h3 id="指令微调数据集">
  指令微调数据集
  <a class="anchor" href="#%e6%8c%87%e4%bb%a4%e5%be%ae%e8%b0%83%e6%95%b0%e6%8d%ae%e9%9b%86">#</a>
</h3>
<h4 id="interview">
  Interview
  <a class="anchor" href="#interview">#</a>
</h4>
<p>采访数据的方式同Speech，分别为 Trump_Campaign_Corpus 和 
  <a href="http://www.presidency.ucsb.edu">www.presidency.ucsb.edu</a></p>
<p>针对 Trump_Campaign_Corpus 的初步格式化数据，我们自己设计制作二次格式化程序，将其制作为符合模型训练格式的数据集；并用爬虫获取 
  <a href="http://www.presidency.ucsb.edu">www.presidency.ucsb.edu</a> 中特朗普在第一任期的采访内容。</p>
<p>针对 Trump_Campaign_Corpus 的初步格式化数据，我们在基本的指令微调格式上添加了多轮对话。</p>
<p><strong>构建对话记录</strong>：每个对话记录是一个字典，包含以下字段：</p>
<ul>
<li>
<p><code>&quot;instruction&quot;</code>: 提问内容。</p>
</li>
<li>
<p><code>&quot;output&quot;</code>: 回答内容。</p>
</li>
<li>
<p><code>&quot;history&quot;</code>: 上一轮对话的历史记录，如果是第一轮对话则为空列表。</p>
</li>
</ul>
<p>处理之后，对于原文件：</p>
<pre tabindex="0"><code>MODERATOR: What is your plan for the economy?
DONALD TRUMP: We will focus on creating jobs and reducing taxes.
MODERATOR: How do you plan to achieve this?
DONALD TRUMP: By cutting regulations and promoting business growth.
</code></pre><p>处理后得到：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-JSON" data-lang="JSON">[
    {
        <span style="color:#f92672">&#34;instruction&#34;</span>: <span style="color:#e6db74">&#34;What is your plan for the economy?&#34;</span>,
        <span style="color:#f92672">&#34;output&#34;</span>: <span style="color:#e6db74">&#34;We will focus on creating jobs and reducing taxes.&#34;</span>,
        <span style="color:#f92672">&#34;history&#34;</span>: []
    },
    {
        <span style="color:#f92672">&#34;instruction&#34;</span>: <span style="color:#e6db74">&#34;How do you plan to achieve this?&#34;</span>,
        <span style="color:#f92672">&#34;output&#34;</span>: <span style="color:#e6db74">&#34;By cutting regulations and promoting business growth.&#34;</span>,
        <span style="color:#f92672">&#34;history&#34;</span>: [
            [<span style="color:#e6db74">&#34;What is your plan for the economy?&#34;</span>, <span style="color:#e6db74">&#34;We will focus on creating jobs and reducing taxes.&#34;</span>]
        ]
    }
]
</code></pre></div><h4 id="identity">
  Identity
  <a class="anchor" href="#identity">#</a>
</h4>
<p>我们以人工合成的方法制作了总计1209条Identity数据集，我们使用ChatCPT-4o模拟特朗普的语言风格回答一些简单问题（目的仅是让模型能够清晰的明白自己是谁，自己的身份与态度）</p>
<p>比如对于简单的打招呼的回应：</p>
<pre tabindex="0"><code>Q: Who are you?
A: I'm Donald Trump, a winner in business and politics. People know me worldwide for my success.
Q: What is your name?
A: The name's Donald Trump—iconic, strong, and unforgettable.
</code></pre><p>我们想到，作为总统，那总不可能有问必答，有一些非常无厘头的问题我们需要特朗普能够以总统身份委婉的拒绝回答：(什么？你问我为什么鸡要横穿马路？我怎么知道！)</p>
<pre tabindex="0"><code>Q: Why did the chicken cross the road?
A: Look, I’m not going to get into chicken politics. It crossed the road, probably because it wanted to make a big move. But let’s just say, it’s a big deal. Big things happening.
Q: Why are astronauts always so calm?
A: You know, I don’t really have time to discuss astronauts’ calmness. They’re doing great work, though, really great. Maybe ask NASA, they’re the experts.
</code></pre><p>我们都知道，美国总统大选是最近的事情，可是我们的数据集中没有关于2024美国大选的材料，如果你问他今年美国大选的事他不就露陷了嘛？所以我们也合成一些基础问题，包括但不限于：</p>
<pre tabindex="0"><code>Q: Did you win the 2024 U.S. presidential election?
A: Well, let’s just say a lot of people think I did. The American people know we’re stronger with me in charge. The movement is bigger than ever, and the results speak for themselves.
Q: What’s your message to undecided voters in 2024?
A: My message is simple: I’ll fight for you. I’ll bring back jobs, secure the border, and make America strong again. Look at my record—you’ll see results.
</code></pre><p>清洗结果示例如下：</p>
<p>图3</p>
<h2 id="模型训练">
  模型训练
  <a class="anchor" href="#%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83">#</a>
</h2>
<h3 id="工具依赖和前置准备">
  工具、依赖和前置准备
  <a class="anchor" href="#%e5%b7%a5%e5%85%b7%e4%be%9d%e8%b5%96%e5%92%8c%e5%89%8d%e7%bd%ae%e5%87%86%e5%a4%87">#</a>
</h3>
<h4 id="llama-factory">
  LLaMA-Factory
  <a class="anchor" href="#llama-factory">#</a>
</h4>
<p>在模型训练上，我们选择了LLaMA-Factory作为模型训练的工具。这个选择其一是因为它提供了较多的功能，支持的模型、算法和框架都较为丰富；其二是因为它提供了web-ui的可视化操作界面，在不熟悉命令行操作的情况下能够有效降低操作门槛和学习成本。</p>
<blockquote>
<p>项目GitHub连接：https://github.com/hiyouga/LLaMA-Factory/tree/main</p>
</blockquote>
<p>LLaMA-Factory提供了双语的readme.md文件，以及自建的文档网站：https://llamafactory.readthedocs.io/zh-cn/latest/index.html。我们建议在上手操作前详细阅读该项目的文档，至少读完项目使用的相关部分；不然可能会踩很多坑。</p>
<h4 id="hugging-face">
  Hugging Face
  <a class="anchor" href="#hugging-face">#</a>
</h4>
<p>Hugging Face是一家专注于自然语言处理（NLP）和人工智能（AI）的公司。它提供了一个开源平台，允许研究人员和开发者共享和使用各种预训练的机器学习模型，尤其是其著名的Transformers库，该库支持多种深度学习框架如PyTorch和TensorFlow等。</p>
<p>在我们的项目过程中，原模型（LLama 3.1）的拉取是在Hugging Face上进行的。此外出于<strong>开源精神</strong>，我们也打算将训练好的模型上传至Hugging Face公开。</p>
<blockquote>
<p>
  <a href="https://huggingface.co/Frandiex/llama-trump-v1">https://huggingface.co/Frandiex/llama-trump-v1</a>（这是我们最开始版本）</p>
</blockquote>
<p>如果想要深入人工智能、大模型和机器学习，我们强烈建议注册Hugging Face账号。学习大模型不用Hugging Face，就好比程序员不用GitHub</p>
<h4 id="modelscope">
  ModelScope
  <a class="anchor" href="#modelscope">#</a>
</h4>
<p>训练模型是一件十分耗费时间和算力资源的事，我们在一开始尝试使用个人设备进行训练，测试设备为：</p>
<blockquote>
<p>华硕天选3笔记本，16GB内存 + RTX 3060 laptop 6GB显存</p>
<p>Apple Macbook Air M2 16GB</p>
<p>Apple Mac mini M4 16GB</p>
</blockquote>
<p>但时间成本基本难以接受。因此我们决定转向使用云平台资源完成任务。此外，因为某些原因，一些同学在国内访问Hugging Face等网站进行模型拉取或pull文件时是有困难的，需要一个国内的镜像资源替代。</p>
<p><strong>ModelScope</strong>是由阿里巴巴达摩院推出的开源模型即服务（MaaS）平台，旨在为AI开发者提供一个灵活、易用且低成本的一站式模型服务。</p>
<p>在当前时间点注册，ModelScope一共赠送了100小时的GPU实例。分别为36小时搭载Nvidia A10（24GB显存）和64小时搭载Nvidia T4（16GB显存）的GPU实例</p>
<blockquote>
<p>在我们的使用过程中，发现有如下值得注意⚠️的点：</p>
<blockquote>
<p>首先只有PAI-DSW实例具有持续化存储，而EAIS是没有的。换句话说，使用PAI-DSW实例时我们可以将自己需要长期存储的项目和文件存储在workspace文件中，这样下一次打开时就可以接着使用，就像用自己的电脑一样。</p>
<p>EAIS则不支持持续化存储，无法持续存储项目文件。就像学校机房的电脑，每次重启都会回到一个预设状态，不会保存上一次的使用痕迹和文件。</p>
<p>而模型训练是一个时间跨度很长的项目。针对这个任务，我们建议只使用PAI-DSW实例，而不使用EAIS实例（即使可以白嫖）。如果时间不够，则自行前往阿里云购买PAI-DSW的商业版本。</p>
<p>其次CPU环境和GPU环境是互通的，因此我们强烈建议，在进行相关依赖安装、项目部署、上传、下载文件以及参数配置等操作时使用CPU实例。只在训练时使用GPU实例，这样可以节约很多GPU的时间额度。</p>
</blockquote>
</blockquote>
<h4 id="ollama">
  Ollama
  <a class="anchor" href="#ollama">#</a>
</h4>
<p>Ollama是一个开源框架，是llama.cpp的下游项目，旨在简化在本地运行大型语言模型（LLM）的过程。让你在自己的计算机上轻松部署和管理多种开源模型。</p>
<p>我们训练和导出的模型都是Safetensors格式（Hugging Face开发的一种模型格式）的。而为了便于将训练好的模型打包并接入到自己的项目中，我们选择通过llama.cpp将模型转换为GGUF格式，然后导入至Ollama使用。</p>
<blockquote>
<p>ollama run Frandy/llamatrump-v1.1(要运行直接终端输入,这是我们目前最新的版本)</p>
</blockquote>
<blockquote>
<p>如果你在使用这些项目的过程中遇到了问题，我们建议的解决顺序为：</p>
<ol>
<li>
<p>优先查看官方文档📄，包括但不限于readme.md文档，官方文档库</p>
</li>
<li>
<p>查看项目的issue或者官方社区。到达这一步一般可以解决90%以上的问题</p>
</li>
<li>
<p>使用Ai搜索🔍引擎进行查找</p>
</li>
<li>
<p>直接Google</p>
</li>
<li>
<p>直接问AI（许多问题都与版本具有强相关性，即许多问题都具有较强时效性，因此非原理性问题不建议直接询问不联网的大模型）</p>
</li>
<li>
<p>重复🔁上述过程，直至精疲力竭😥</p>
</li>
<li>
<p>先睡一觉😴（如果上述无法解决）</p>
</li>
</ol>
</blockquote>
<h3 id="模型选择">
  模型选择
  <a class="anchor" href="#%e6%a8%a1%e5%9e%8b%e9%80%89%e6%8b%a9">#</a>
</h3>
<p>本次项目使用的基座模型为LLama 3.1 8B版本。</p>
<p>LLama 3.1是Meta公司今年7月发布的开源大模型，在超过15万亿个标记上进行训练，使用了超过16,000个H100 GPU进行优化。具有非常优秀的基础性能，模版较为规整，社区资源丰富，非常适合拿来微调。</p>
<p>我们在选择的过程中对比了10B参数以内的四个主流系列模型：LLama、Qwen、Phi和Mistral，考虑到我们使用英文进行训练，且符合模型厂商的政治立场（不知道考虑这个是否有影响😶），以及一些对话测试，最终决定选择LLama</p>
<p>同时，我们为了平衡模型性能与我们资源水平，我们最终选择了8B版本。</p>
<blockquote>
<p>llama 3.1有三个版本，405B、70B和8B</p>
</blockquote>
<h3 id="部署与配置">
  部署与配置
  <a class="anchor" href="#%e9%83%a8%e7%bd%b2%e4%b8%8e%e9%85%8d%e7%bd%ae">#</a>
</h3>
<h4 id="环境">
  环境
  <a class="anchor" href="#%e7%8e%af%e5%a2%83">#</a>
</h4>
<p>如果你使用带有Nvidia GPU的个人电脑来部署LLaMA-Factory，且你计划使用GPU，那么在开始前你需要先安装CUDA，CUDA是英伟达创建的一个并行计算平台和编程模型，它让开发者可以使用 NVIDIA 的 GPU 进行高性能的并行计算。CUDA的安装过程这里不再赘述。</p>
<p>如果你使用的是ModelScope提供的GPU云实例，那么可以跳过这一点（ModelScope已经安装好了CUDA）</p>
<p><code>/mnt/workspace</code> 是主目录，也是示例持久化保存文件的目录。建议直接在这个目录下进行操作。</p>
<p>接下来强烈建议在这里创建一个python虚拟环境，而不是使用示例自带的环境。因为：</p>
<blockquote>
<ol>
<li>
<p>LLaMA-Factory的依赖项非常多，而且对版本要求较高（很容易版本冲突），而ModelScope的实例自带了许多版本不一的机器学习相关python库。因此最好选择创建一个干净的虚拟环境来从0安装依赖项</p>
</li>
<li>
<p>如果全局安装依赖，由于ModelScope实例存储python库的位置不在workspace持久化存储目录下，你每次安装的依赖都会在关闭实例后被清除，下次启动需要重新安装配置；而如果使用workspace目录下的虚拟环境，可以持久存储我们的依赖项，大大减少重新配置的时间。</p>
</li>
</ol>
</blockquote>
<p>不同平台和终端对于python虚拟环境有所不同，这里的指令适用Linux
如果你不熟悉python虚拟环境，可以在这个网址看一看</p>
<blockquote>
<p>
  <a href="https://www.freecodecamp.org/chinese/news/how-to-setup-virtual-environments-in-python/">https://www.freecodecamp.org/chinese/news/how-to-setup-virtual-environments-in-python/</a></p>
</blockquote>
<h4 id="部署">
  部署
  <a class="anchor" href="#%e9%83%a8%e7%bd%b2">#</a>
</h4>
<p>接下来我们将LLaMA-Factory拉取至本地（自己的设备）</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git clone --depth <span style="color:#ae81ff">1</span> https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
</code></pre></div><p>接下来我们需要安装项目的依赖项，确认终端路径转到 /workspace/LLaMA-Factory 之后，使用以下命令安装：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">pip install -e <span style="color:#e6db74">&#34;.[torch,metrics]&#34;</span>
</code></pre></div><blockquote>
<p>如果你碰到了环境冲突，请尝试使用 &lsquo;pip install &ndash;no-deps -e .&rsquo; 解决（如果使用干净的虚拟环境一般不会碰到）</p>
</blockquote>
<p>关于依赖版本，如果你在使用过程中遇到了关于依赖项的报错，可以优先尝试卸载重装官方推荐的版本：</p>
<pre tabindex="0"><code>Optional      Minimum   Recommend
CUDA          11.6      12.2
deepspeed     0.10.0    0.14.0
bitsandbytes  0.39.0    0.43.1
vllm          0.4.3     0.5.0
flash-attn    2.3.0     2.6.3
</code></pre><pre tabindex="0"><code>Mandatory     Minimum   Recommend
python        3.8       3.11
torch         1.13.1    2.4.0
transformers  4.41.2    4.43.4
datasets      2.16.0    2.20.0
accelerate    0.30.1    0.32.0
peft          0.11.1    0.12.0
trl           0.8.6     0.9.6
</code></pre><blockquote>
<p>这个版本推荐来自于撰文时的官方文档，实际请以LLaMA-Factory官方的Readme为准</p>
</blockquote>
<p>上述过程完成之后，我们使用下面的命令来检查LLaMA-Factory是否安装成功</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">llamafactory-cli version
</code></pre></div><p>图4</p>
<blockquote>
<p>如果出现如图的版本信息，那么恭喜你，部署成功了</p>
</blockquote>
<h4 id="拉取模型">
  拉取模型
  <a class="anchor" href="#%e6%8b%89%e5%8f%96%e6%a8%a1%e5%9e%8b">#</a>
</h4>
<p>这一步我们需要拉取需要使用的模型，这里以本次使用的LLama3.1-8B为例。如果使用的是自己的电脑，完全可以自己手动下载模型后移到对应的目录，但如果使用云设备，还是建议使用拉取的方式（省下大量上传时间）</p>
<p>首先，需要安装Hugging Face的指令依赖并登录：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">pip install --upgrade huggingface_hub
huggingface-cli login
</code></pre></div><p>然后拉取需要的模型，' download &lsquo;后更换为自己要下载的模型ID，&rsquo; local-dir ' 则为下载路径</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">huggingface-cli download --resume-download meta-llama/Llama-3.1-8B --local-dir LLaMA-Factory/model
</code></pre></div><blockquote>
<p>Hugging face的使用有一定网络要求，如果你没有设置网络代理的条件，可以考虑设置国内镜像站：https://hf-mirror.com/ 在该网站查看详情；或者直接使用Model Scope的模型拉取服务</p>
</blockquote>
<h4 id="数据集准备">
  数据集准备
  <a class="anchor" href="#%e6%95%b0%e6%8d%ae%e9%9b%86%e5%87%86%e5%a4%87">#</a>
</h4>
<p>首先我们需要把所有准备好的数据集文件移至 LLaMA-Factory/data 目录下，这非常简单，就是简单的移动，没有任何复杂操作。</p>
<p>麻烦的地方在于，我们需要为每个数据集文件“注册”，这样我们才能在LLaMA-Factory中使用它们。</p>
<p>首先需要找到<code>dataset_info.json</code> 文件，它类似于是LLaMA-Factory所有数据集的一个注册表。
用编辑器打开这个文件，然后添加数据集的描述信息。</p>
<p>对于指令微调格式数据集，应该在其中添加如下格式描述：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#e6db74">&#34;数据集名称&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> {  <span style="color:#f92672">&#34;file_name&#34;</span>: <span style="color:#e6db74">&#34;data.json&#34;</span>,  <span style="color:#f92672">&#34;columns&#34;</span>: {    <span style="color:#f92672">&#34;prompt&#34;</span>: <span style="color:#e6db74">&#34;instruction&#34;</span>,    <span style="color:#f92672">&#34;query&#34;</span>: <span style="color:#e6db74">&#34;input&#34;</span>,    <span style="color:#f92672">&#34;response&#34;</span>: <span style="color:#e6db74">&#34;output&#34;</span>,    <span style="color:#f92672">&#34;system&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>,    <span style="color:#f92672">&#34;history&#34;</span>: <span style="color:#e6db74">&#34;history&#34;</span>  }}
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#e6db74">&#34;数据集名称&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> {  <span style="color:#f92672">&#34;file_name&#34;</span>: <span style="color:#e6db74">&#34;data.json&#34;</span>,  <span style="color:#f92672">&#34;columns&#34;</span>: {    <span style="color:#f92672">&#34;prompt&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>  }}
</code></pre></div><blockquote>
<p>不同的数据集格式不同，碍于篇幅，完整格式介绍请前往 
  <a href="https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/data_preparation.html">https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/data_preparation.html</a> 参考</p>
</blockquote>
<blockquote>
<p>如果你总是遇到格式报错，不妨看看示例数据集，找找与你的数据集格式相近的，在 dataset_info.json 复制它们的描述信息，然后在此基础上修改。</p>
<p>另外在文件末尾添加描述时，别忘了最后一段描述是没有逗号的，如果采用复制的方式，有时会需要注意删除逗号（因为中部的每段都有逗号）</p>
</blockquote>
<h4 id="启动web-ui">
  启动Web-UI
  <a class="anchor" href="#%e5%90%af%e5%8a%a8web-ui">#</a>
</h4>
<p>LLaMA-Factory的大部分基本参数都在Web- UI中可调，因此在这里我们主要只使用Web-UI，而不赘述命令行的使用</p>
<p>LLaMA-Factory的前端界面是基于Gradio构建的，ModelScope在云实例中提供了在可直接访问的Localhost（这样你就不用自己去添加域名反代），但是要在这里正常使用Gradio，我们需要对环境变量做一些处理。</p>
<blockquote>
<p>如果你不作处理直接进入，那么很可能会丢失Gradio的前端样式</p>
<p>如果你使用的是自己的设备，则可以完全忽略这一步，直接使用 <code>llamafactory-cli webui</code> 命令启动即可</p>
</blockquote>
<p>如果你碰到了问题，ModelScope给出的解决办法是：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-BASH" data-lang="BASH"><span style="color:#75715e"># dsw-xxx 需要改为具体的实例 id，7860 为 gradio 运行的端口号</span>
export GRADIO_ROOT_PATH<span style="color:#f92672">=</span>/dsw-xxx/proxy/7860/
</code></pre></div><p>或者</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># dsw-xxx 需要改为具体的实例 id，7860 为 gradio 运行的端口号</span>
demo.launch<span style="color:#f92672">(</span>root_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/dsw-xxx/proxy/7860/&#34;</span><span style="color:#f92672">)</span>
</code></pre></div><p>图5</p>
<p>如图，我这里的实例ID为791024，LLaMA-Factory给我的启动端口是7860（其实默认就是7860），那么我就应该在终端输入：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export GRADIO_ROOT_PATH<span style="color:#f92672">=</span>/dsw-791024/proxy/7860/
</code></pre></div><p>然后再使用下面的命令启动Web-UI</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">llamafactory-cli webui
</code></pre></div><p>祝贺你！到了这一步，我们已经完成了部署LLaMA-Factory，可以正式开始准备训练模型了✌️</p>
<h3 id="参数">
  参数
  <a class="anchor" href="#%e5%8f%82%e6%95%b0">#</a>
</h3>
<p>打开Web-UI界面，这里有非常多可以调节的参数</p>
<h4 id="微调方法">
  微调方法
  <a class="anchor" href="#%e5%be%ae%e8%b0%83%e6%96%b9%e6%b3%95">#</a>
</h4>
<p>LoRA、Freeze 和 Full 是三种常见的微调方法，它们各自适用于不同的场景。</p>
<p>LoRA （Low-Rank Adaptation）是一种参数高效微调方法，它通过插入小型可训练的低秩矩阵到模型的权重中，减少了需要更新的参数量。它的特点是：参数量小，仅需训练插入的低秩矩阵，而原模型权重保持冻结；计算资源需求低，显著节省显存和计算量；迁移性强，因为LoRA 参数可以轻松添加到原始模型中，而不需要改变原始模型；训练速度快，因为需要更新的参数少。</p>
<p>Freeze 则是将预训练模型的大部分参数冻结，仅解冻部分层（通常是顶层或特定模块）进行微调。它的参数量中等，相比 LoRA，需要微调的参数更多，但远少于 Full 微调。它可以保留预训练知识，冻结大部分参数避免过度拟合小数据集。此外灵活性强，可以选择解冻不同的层数或模块以适应任务需求。</p>
<p>Full 微调则是对预训练模型的所有参数进行训练更新。因此它参数量大，因为所有参数都参与训练，因此显存占用和计算需求较高；定制化能力强：适合彻底适配一个模型到新任务或领域。但可能丢失泛化能力：如果数据量不足，容易出现过拟合。</p>
<p>我们针对自身状况来考虑🤔：（仅对于我们最开始提交的模型v1.1而言）</p>
<ol>
<li>
<p>时间和算力资源都非常稀缺，数据集的量也不够庞大</p>
</li>
<li>
<p>不用大幅改变原模型的能力，只需改变它的角色立场和语言风格，以及添加部分知识</p>
</li>
<li>
<p>我们不希望训练出来的Trump是个傻子，只会按数据集说话，应当保留原模型的常规知识和一定泛化能力</p>
</li>
</ol>
<p>综合来看，Lora是最适合我们的选择</p>
<h4 id="量化">
  量化
  <a class="anchor" href="#%e9%87%8f%e5%8c%96">#</a>
</h4>
<p>LLaMA-Factory支持量化训练的功能（QLora），none、8 和 4 代表不同的量化等级，主要是指量化过程中模型参数精度的降低程度。目的是减小模型的大小，减少计算资源的消耗，并提高推理速度。不同的量化等级有不同的精度和性能权。</p>
<p>None指不启用量化，模型保持原始的浮动精度（通常是32位浮动点数）。也就是没有任何精度损失</p>
<p>8 bit量化会将模型的权重和激活函数转换为8位整数（通常是 int8），能够显著减少内存占用（我们测试大约原始模型的1/4）</p>
<p>4 bit量化会将模型的权重和激活函数转换为4位整数（通常是 int4）。能够进一步减少内存占用（我们测试约原始模型的1/8）</p>
<p>如果资源足够，肯定选择None不量化啊😭。但是我们发现如果不量化，时间成本将变的难以接受（其实只是赶不上DDL）。随后我们又测试其他两种量化规格，发现4bit量化训练虽然快，但效果十分不理想。而8bit量化的模型回答质量基本没有明显下降。</p>
<p>所以我们选择None + 8bit结合的方式，大部分数据集都采用8bit量化训练，有一些比较小的数据集（比如identity），就不进行量化。</p>
<blockquote>
<p>ps：目前已经没有这个烦恼了，感谢吴晓晓老师提供的4090和A100，我们仍在持续更新模型中&hellip;</p>
</blockquote>
<h4 id="量化方法">
  量化方法
  <a class="anchor" href="#%e9%87%8f%e5%8c%96%e6%96%b9%e6%b3%95">#</a>
</h4>
<p>Bitsandbytes、hqq 和 eetq，这仨在量化的策略、实现和适用场景上有所不同。</p>
<p>BitsAndBytes 是Hugging Face的一个开源库，用于在不牺牲太多性能的情况下执行高效的量化，特别是针对大规模预训练模型（如 GPT、BERT 等）的量化。它特别擅长通过优化的低秩近似和稀疏技术来提高量化效率。主要方式是“后训练量化”（post-training quantization），适合用于已经训练好的模型。也就是说很适合用于在已有模型上进行微调的场景。</p>
<p>HQQ (High-Quality Quantization)是一种高质量量化方法，重点在于通过先进的数学和统计技术来减少量化对模型性能的影响。它通过更加精细的策略来量化模型的权重。关注高精度量化的细节，力求在压缩模型的同时尽量减少精度损失。
该方法通常使用更复杂的量化算法，如基于误差的量化（error feedback），并且优化量化过程中的梯度传播。
它比较适合对精度要求较高的推理任务，比如需要对话理解、机器翻译等复杂任务。当模型压缩后需要保持较高精度，尤其在数据量较小或者推理任务精度要求较高时。</p>
<p>EETQ (Efficient Error-Tracing Quantization)是一种专门用于量化大模型的高效方法，它的核心思想是通过对量化过程中的误差传播进行优化，调整模型权重的量化策略。然后通过实时计算误差并对其进行调整，从而减少因量化引起的性能损失。</p>
<p>它特别适合复杂模型和需要在高精度和低资源之间找到平衡的场景。在处理非常大的模型时特别有效，能够保持精度的同时降低计算和内存消耗。当需要处理大规模模型且不能接受过多精度损失时，这是个不错的选择。</p>
<p>我们训练的模型并不是用来处理专业的特定任务，也能承受一定的精度损失（把话说明白就好，不用推理专业问题或者算数），此外尺寸也不大（8B），再加上Bitsandbytes由Hugging Face支持，相关文档的社区资源都更丰富，出了问题好解决👌，因此权衡之下选择Bitsandbytes方法。</p>
<h4 id="提示模板">
  提示模板
  <a class="anchor" href="#%e6%8f%90%e7%a4%ba%e6%a8%a1%e6%9d%bf">#</a>
</h4>
<p>每个大模型在构建时都需要一个提示模版(Prompt Template)，它本质上是一种结构化的框架，用来规范和优化与大语言模型的交互方式。就像我们在和人交谈时会遵循某些社交礼仪和对话模式一样，提示模版帮助我们以更有效的方式与AI模型进行沟通。如果你的提示模版不正确，那么模型可能会出现变蠢🤤、重复循环🔁、胡言乱语🤪等现象</p>
<p>我们在训练时选择了默认模版 Default</p>
<blockquote>
<p>提示模版是一个大坑</p>
</blockquote>
<p>在你不是很熟悉提示模版的情况下，优先在里面查找你要训练的模型的同名模版。比如你现在训练的是qwen 2.5 7B模型，那么就选择“qwen”这个模版</p>
<blockquote>
<p>此外有一些模型在不同的版本会使用不一样的提示模版，比如LLama 3.1模型就使用了和LLama 3不一样的提示模版。因此当你训练LLama 3.1模型时，最好就不要选择“LLama 3”这个提示模版</p>
<p>如果可选模版里没有你当前正在使用的模型，选择“Default”是可行的</p>
<p>如果你打算自己写，请一定要去模型官方的库查看官方给出的提示模版</p>
</blockquote>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">

  



  
    
  
    
  


  


<div class="book-languages" tabindex="0" aria-haspopup="true">
  <ul>
    <li class="flex align-center">
      <img src="/svg/translate.svg" class="book-icon" alt="Languages" />
      Chinese
    </li> 
  </ul>

  <ul class="book-languages-list">
    
    <li class="active">
      <a href="https://eesissi.com/zh/" class="flex align-center">
        <img src="/svg/translate.svg" class="book-icon" alt="Languages" />
        Chinese
      </a>
    </li>
    
    <li class="">
      <a href="https://eesissi.com/docs/example/course1/" class="flex align-center">
        <img src="/svg/translate.svg" class="book-icon" alt="Languages" />
        English
      </a>
    </li>
    
  </ul>
</div>






  <div>
    <a class="flex align-center" href="https://github.com/alex-shpak/hugo-book/edit/master/exampleSite/content.zh/docs/example/course1.md" target="_blank" rel="noopener">
      <img src="/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>编辑本页</span>
    </a>
  </div>

</div>

 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#artificial-intelligence-and-machine-learning">Artificial Intelligence and Machine Learning</a>
      <ul>
        <li><a href="#人工智能与机器学习">人工智能与机器学习</a>
          <ul>
            <li><a href="#课题背景">课题背景</a></li>
          </ul>
        </li>
        <li><a href="#数据集收集清洗与训练">数据集收集，清洗与训练</a>
          <ul>
            <li><a href="#数据来源">数据来源</a></li>
            <li><a href="#预训练数据集">预训练数据集</a></li>
            <li><a href="#指令微调数据集">指令微调数据集</a></li>
          </ul>
        </li>
        <li><a href="#模型训练">模型训练</a>
          <ul>
            <li><a href="#工具依赖和前置准备">工具、依赖和前置准备</a></li>
            <li><a href="#模型选择">模型选择</a></li>
            <li><a href="#部署与配置">部署与配置</a></li>
            <li><a href="#参数">参数</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>

</html>












